{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loi_f63soYVr"
   },
   "source": [
    "# CSML1010 Project Working Copy\n",
    "# Sentiment Analysis with the Sentiment140 dataset\n",
    "## Pete Gray - YorkU #217653247\n",
    "\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project we explore and learn about natural language processing and the lifecycle of machine learning projects. Sentiment analysis will be performed using the Sentiment140 dataset[1]. We will explicity execute data cleaning, data exploration, feature engineering, feature selection, modeling, model selection, and [OTHER STUFF?] The code is mostly assembled from bits and pieces of the coding exercises that are part of the course CSML1010, Fall 2019, at York University, Toronto.\n",
    "\n",
    "[1] http://help.sentiment140.com/for-students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TABLE OF CONTENTS\n",
    "\n",
    "1. Load data and libraries\n",
    "2. Data Cleaning\n",
    "3. Data Exploration\n",
    "4. Feature Engineering\n",
    "5. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4NUyQZQnSBp"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i72mPmFnSBq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import model_evaluation_utils as meu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdxmVNijmdOs"
   },
   "source": [
    "# Adjust pandas display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTTBG34mmWF0"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.precision = 2\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DB0TDJgcm8bj"
   },
   "source": [
    "# Import matplotlib and seaborn and adjust defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFdcwFgZm6Zl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjQmJLg6nSBw"
   },
   "source": [
    "## Read data from local filesystem and csv source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T18:25:50.316246Z",
     "start_time": "2018-12-11T18:25:37.226960Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4TSsbsKqnSBy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MM12Q5qNnSB2"
   },
   "source": [
    "Check data with quick visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "ALDoZOPAnSB4",
    "outputId": "2b0b817e-5dce-40ac-df23-e2be2169a50d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599994</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599995</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599996</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599997</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599998</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "0        scotthamilton     \n",
       "1        mattycus          \n",
       "2        ElleCTF           \n",
       "3        Karoli            \n",
       "4        joy_wolf          \n",
       "...           ...          \n",
       "1599994  AmandaMarie1028   \n",
       "1599995  TheWDBoards       \n",
       "1599996  bpbabe            \n",
       "1599997  tinydiamondz      \n",
       "1599998  RyanTrevMorris    \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!     \n",
       "1        @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                           \n",
       "2        my whole body feels itchy and like its on fire                                                                      \n",
       "3        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.      \n",
       "4        @Kwesidei not the whole crew                                                                                        \n",
       "...                                ...                                                                                       \n",
       "1599994  Just woke up. Having no school is the best feeling ever                                                             \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta                                      \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me for details                                                            \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                                                    \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                                                       \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJhQcLA0u6CJ"
   },
   "source": [
    "## Give dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kS9biLKkt49y"
   },
   "outputs": [],
   "source": [
    "df.columns = ['sentiment', 'ID', 'Time', 'none', 'username', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2CVq6zMunwkN",
    "outputId": "9902caab-b5aa-429b-b572-b6e898af9ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    1599999\n",
       "ID           1599999\n",
       "Time         1599999\n",
       "none         1599999\n",
       "username     1599999\n",
       "Text         1599999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VSn5zWau-3L"
   },
   "source": [
    "### Now it has columns, this seems better.\n",
    "\n",
    "### Check for nulls in the Text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    1599999\n",
       "ID           1599999\n",
       "Time         1599999\n",
       "none         1599999\n",
       "username     1599999\n",
       "Text         1599999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"Text\"].notnull()]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set temporary dataset size, for quicker processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_size = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    3000\n",
       "ID           3000\n",
       "Time         3000\n",
       "none         3000\n",
       "username     3000\n",
       "Text         3000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start_row = int(800000-(dev_data_size/2))-1\n",
    "#finish_row = int(800000+(dev_data_size/2))-1\n",
    "#df_sm = df[start_row:finish_row]\n",
    "df_sm=df.sample(n=dev_data_size,replace=True)\n",
    "df_sm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5gpJp0ULn_mZ",
    "outputId": "10669616-4836-443d-f214-a8e30b637e92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment', 'ID', 'Time', 'none', 'username', 'Text']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [col for col in df.columns if not col.startswith('self')]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@discomaz muy temprano. Necessito chabo though... ',\n",
       "       'with Bff Hind  It was a super nice day  how are u all doing?? 8D',\n",
       "       'No time for the sauna ',\n",
       "       'Funny that people like the post shows better than the podcast itself.  ',\n",
       "       \"&quot;Wipe the cold out my my eye, see who's this paging me and why&quot; Classic song..making the bars harder to guess \",\n",
       "       \"@crauds wow, that's impressive. Our progress is terrible \",\n",
       "       'no good when theres cheesecake in the staff restaurant but i already bought a muller rice ',\n",
       "       '@eagene FB and Twitter on 360 ! Too cool. ',\n",
       "       '@ceciliepoulsen is here ',\n",
       "       'I GET TO SEE KP TOMORROW! omg im a happy girl '], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = np.array(df_sm['Text'])\n",
    "sentiments = np.array(df_sm['sentiment'])\n",
    "raw_text[5:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7udZhKkJnSCI"
   },
   "source": [
    "-----------------------\n",
    "\n",
    "# =======================\n",
    "# Data Cleaning\n",
    "# =======================\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-B4ADb4QCuL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APqmmwo8nSCJ"
   },
   "source": [
    "## Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctg2OlsXnSCK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(s):\n",
    "    s = s.replace(r'<lb>', \"\\n\")\n",
    "    s = s.replace(r'<tab>', \"\\i\")\n",
    "    \n",
    "    # As a sanity check - s = s.replace(r'W', \"Q\")\n",
    "    \n",
    "    s = re.sub(r'<br */*>', \"\\n\", s)\n",
    "    s = s.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    # markdown urls\n",
    "    s = re.sub(r'\\(https*://[^\\)]*\\)', \"\", s)\n",
    "    # normal urls\n",
    "    s = re.sub(r'https*://[^\\s]*', \"\", s)\n",
    "    s = re.sub(r'_+', ' ', s)\n",
    "    s = re.sub(r'\"+', '\"', s)\n",
    "    \n",
    "    # NUMBERS IN THE TEXT DATA\n",
    "    # The numbers in the data came to light during feature engineering. \n",
    "    # I will try different things here.\n",
    "    \n",
    "    # A processor-efficient approach, as suggested at:\n",
    "    # https://stackoverflow.com/questions/30315035/strip-numbers-from-string-in-python\n",
    "    # s = s.translate(None, '0123456789')\n",
    "    # Well, that totally didn't work.\n",
    "    \n",
    "    # From the same link, a more conventional, but less efficient approach:\n",
    "    \n",
    "    s = re.sub(r'\\d+', '', s)\n",
    "    \n",
    "    \n",
    "    # USERNAMES IN THE DATA\n",
    "    # Let's see if life gets any cleaner with these removed, or if it just blows stuff up.\n",
    "    # Using code found at: \n",
    "    # https://stackoverflow.com/questions/50830214/remove-usernames-from-twitter-data-using-python\n",
    "    \n",
    "    s = re.sub('@[^\\s]+','',s) \n",
    "    \n",
    "    # Was 4374 for 2000\n",
    "    # 3593 and 1985\n",
    "    \n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations on the removal of Usernames from data\n",
    "\n",
    "Running with 2000 rows, Bag of Words came up with 4374 dimensions. This took a dog's age to run through RFE. Applying the removal of usernames (strings beginning with '@') from the text data caused 15 of the 2000 to become null - they were stripped from the dataset. Most significantly, it resulted in a reduction of dimensions at the Bag of Words stage to 3593. While this doesn't appear to enable us to ramp up significantly, it does help.\n",
    "\n",
    "It is worth noting that there appears to be a bit less gibberish in the selected features after appying this. So, a little quicker, a little cleaner, it's a keeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmUAz8ConSCP"
   },
   "source": [
    "## Create new column in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0M6eW8LnSCQ"
   },
   "outputs": [],
   "source": [
    "df_sm[\"text_clean\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIWWygCBnSCU"
   },
   "source": [
    "# Iterate and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "177mRPBcnSCU",
    "outputId": "76c4023e-6491-41ba-ad12-9d1b34a313ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1022000\n",
      "processed: 292000\n",
      "processed: 1183000\n"
     ]
    }
   ],
   "source": [
    "for i, row in df_sm.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "            print('processed:'.format(i), i)\n",
    "    df_sm.at[i, \"text_clean\"] = clean(row.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ecr4awlnSCU"
   },
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "ukxXWf7onSCU",
    "outputId": "e2b12d10-743f-47e3-9362-51167b40b130"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>none</th>\n",
       "      <th>username</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>470817</td>\n",
       "      <td>0</td>\n",
       "      <td>2176497332</td>\n",
       "      <td>Mon Jun 15 04:01:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bamboofuchsia</td>\n",
       "      <td>Barely awake. Back and feet ache</td>\n",
       "      <td>Barely awake. Back and feet ache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530967</td>\n",
       "      <td>4</td>\n",
       "      <td>2177924691</td>\n",
       "      <td>Mon Jun 15 06:55:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>classicalgal101</td>\n",
       "      <td>Starts Intro to Digital Photography 2day!</td>\n",
       "      <td>Starts Intro to Digital Photography day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680397</td>\n",
       "      <td>0</td>\n",
       "      <td>2249480800</td>\n",
       "      <td>Fri Jun 19 22:24:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mikeinsd77</td>\n",
       "      <td>Dang I am so close yet so far from 1000 tweets</td>\n",
       "      <td>Dang I am so close yet so far from  tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90589</td>\n",
       "      <td>0</td>\n",
       "      <td>1759077201</td>\n",
       "      <td>Sun May 10 18:24:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JAKAZiD</td>\n",
       "      <td>@smitethis Really sorry I missed you last night, I accidentally slept through and missed the entire Soap Bubble.</td>\n",
       "      <td>Really sorry I missed you last night, I accidentally slept through and missed the entire Soap Bubble.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1167100</td>\n",
       "      <td>4</td>\n",
       "      <td>1980017547</td>\n",
       "      <td>Sun May 31 05:23:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>OzgeCann</td>\n",
       "      <td>@EneStramberg oh u r such a rud man boooooy !!  mickey... I want mickey mouse .. u got me ;))))</td>\n",
       "      <td>oh u r such a rud man boooooy !!  mickey... I want mickey mouse .. u got me ;))))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          ID                          Time      none  \\\n",
       "470817   0          2176497332  Mon Jun 15 04:01:58 PDT 2009  NO_QUERY   \n",
       "1530967  4          2177924691  Mon Jun 15 06:55:32 PDT 2009  NO_QUERY   \n",
       "680397   0          2249480800  Fri Jun 19 22:24:45 PDT 2009  NO_QUERY   \n",
       "90589    0          1759077201  Sun May 10 18:24:33 PDT 2009  NO_QUERY   \n",
       "1167100  4          1980017547  Sun May 31 05:23:28 PDT 2009  NO_QUERY   \n",
       "\n",
       "                username  \\\n",
       "470817   bamboofuchsia     \n",
       "1530967  classicalgal101   \n",
       "680397   mikeinsd77        \n",
       "90589    JAKAZiD           \n",
       "1167100  OzgeCann          \n",
       "\n",
       "                                                                                                                      Text  \\\n",
       "470817   Barely awake. Back and feet ache                                                                                    \n",
       "1530967  Starts Intro to Digital Photography 2day!                                                                           \n",
       "680397   Dang I am so close yet so far from 1000 tweets                                                                      \n",
       "90589    @smitethis Really sorry I missed you last night, I accidentally slept through and missed the entire Soap Bubble.    \n",
       "1167100  @EneStramberg oh u r such a rud man boooooy !!  mickey... I want mickey mouse .. u got me ;))))                     \n",
       "\n",
       "                                                                                                      text_clean  \n",
       "470817   Barely awake. Back and feet ache                                                                         \n",
       "1530967  Starts Intro to Digital Photography day!                                                                 \n",
       "680397   Dang I am so close yet so far from  tweets                                                               \n",
       "90589     Really sorry I missed you last night, I accidentally slept through and missed the entire Soap Bubble.   \n",
       "1167100   oh u r such a rud man boooooy !!  mickey... I want mickey mouse .. u got me ;))))                       "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional pre-processing: tokenization, removing extra whitespaces, lower casing and more advanced operations like spelling corrections, grammatical error corrections, removing repeated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm[\"text_normalized\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 1022000\n",
      "processed: 292000\n",
      "processed: 1183000\n"
     ]
    }
   ],
   "source": [
    "for i, row in df_sm.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "            print('processed:'.format(i), i)\n",
    "    df_sm.at[i, \"text_normalized\"] = normalize_corpus(row.text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>none</th>\n",
       "      <th>username</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>470817</td>\n",
       "      <td>0</td>\n",
       "      <td>2176497332</td>\n",
       "      <td>Mon Jun 15 04:01:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bamboofuchsia</td>\n",
       "      <td>Barely awake. Back and feet ache</td>\n",
       "      <td>Barely awake. Back and feet ache</td>\n",
       "      <td>barely awake back feet ache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530967</td>\n",
       "      <td>4</td>\n",
       "      <td>2177924691</td>\n",
       "      <td>Mon Jun 15 06:55:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>classicalgal101</td>\n",
       "      <td>Starts Intro to Digital Photography 2day!</td>\n",
       "      <td>Starts Intro to Digital Photography day!</td>\n",
       "      <td>starts intro digital photography day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680397</td>\n",
       "      <td>0</td>\n",
       "      <td>2249480800</td>\n",
       "      <td>Fri Jun 19 22:24:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mikeinsd77</td>\n",
       "      <td>Dang I am so close yet so far from 1000 tweets</td>\n",
       "      <td>Dang I am so close yet so far from  tweets</td>\n",
       "      <td>dang close yet far tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90589</td>\n",
       "      <td>0</td>\n",
       "      <td>1759077201</td>\n",
       "      <td>Sun May 10 18:24:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JAKAZiD</td>\n",
       "      <td>@smitethis Really sorry I missed you last night, I accidentally slept through and missed the entire Soap Bubble.</td>\n",
       "      <td>Really sorry I missed you last night, I accidentally slept through and missed the entire Soap Bubble.</td>\n",
       "      <td>really sorry missed last night accidentally slept missed entire soap bubble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1167100</td>\n",
       "      <td>4</td>\n",
       "      <td>1980017547</td>\n",
       "      <td>Sun May 31 05:23:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>OzgeCann</td>\n",
       "      <td>@EneStramberg oh u r such a rud man boooooy !!  mickey... I want mickey mouse .. u got me ;))))</td>\n",
       "      <td>oh u r such a rud man boooooy !!  mickey... I want mickey mouse .. u got me ;))))</td>\n",
       "      <td>oh u r rud man boooooy mickey ... want mickey mouse .. u got ;))))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863898</td>\n",
       "      <td>4</td>\n",
       "      <td>1677188206</td>\n",
       "      <td>Sat May 02 00:51:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dalispacheco</td>\n",
       "      <td>Finally I am free!!! the bimonthly stressful exams are over and I can sleep without anyone bothering me!!!!</td>\n",
       "      <td>Finally I am free!!! the bimonthly stressful exams are over and I can sleep without anyone bothering me!!!!</td>\n",
       "      <td>finally free ! bimonthly stressful exams sleep without anyone bothering !!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1581321</td>\n",
       "      <td>4</td>\n",
       "      <td>2190173341</td>\n",
       "      <td>Tue Jun 16 01:58:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>luvpups77</td>\n",
       "      <td>YAY!!! i am done unpacking  room is neat and tidy</td>\n",
       "      <td>YAY!!! i am done unpacking  room is neat and tidy</td>\n",
       "      <td>yay ! done unpacking room neat tidy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1359903</td>\n",
       "      <td>4</td>\n",
       "      <td>2048555604</td>\n",
       "      <td>Fri Jun 05 15:23:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>julie_bergmann</td>\n",
       "      <td>@TeamMileycyrusx I learned the guitar by myself, it's easy if you just get into it</td>\n",
       "      <td>I learned the guitar by myself, it's easy if you just get into it</td>\n",
       "      <td>learned guitar easy get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1484527</td>\n",
       "      <td>4</td>\n",
       "      <td>2067684873</td>\n",
       "      <td>Sun Jun 07 12:46:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BrittCostanzo</td>\n",
       "      <td>Just a got Twitter</td>\n",
       "      <td>Just a got Twitter</td>\n",
       "      <td>got twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437717</td>\n",
       "      <td>0</td>\n",
       "      <td>2065991707</td>\n",
       "      <td>Sun Jun 07 09:40:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AQuietMadness</td>\n",
       "      <td>@jordanknight The pg won't load for me.</td>\n",
       "      <td>The pg won't load for me.</td>\n",
       "      <td>pg wont load</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          ID                          Time      none  \\\n",
       "470817   0          2176497332  Mon Jun 15 04:01:58 PDT 2009  NO_QUERY   \n",
       "1530967  4          2177924691  Mon Jun 15 06:55:32 PDT 2009  NO_QUERY   \n",
       "680397   0          2249480800  Fri Jun 19 22:24:45 PDT 2009  NO_QUERY   \n",
       "90589    0          1759077201  Sun May 10 18:24:33 PDT 2009  NO_QUERY   \n",
       "1167100  4          1980017547  Sun May 31 05:23:28 PDT 2009  NO_QUERY   \n",
       "...     ..                 ...                           ...       ...   \n",
       "863898   4          1677188206  Sat May 02 00:51:13 PDT 2009  NO_QUERY   \n",
       "1581321  4          2190173341  Tue Jun 16 01:58:50 PDT 2009  NO_QUERY   \n",
       "1359903  4          2048555604  Fri Jun 05 15:23:33 PDT 2009  NO_QUERY   \n",
       "1484527  4          2067684873  Sun Jun 07 12:46:46 PDT 2009  NO_QUERY   \n",
       "437717   0          2065991707  Sun Jun 07 09:40:59 PDT 2009  NO_QUERY   \n",
       "\n",
       "                username  \\\n",
       "470817   bamboofuchsia     \n",
       "1530967  classicalgal101   \n",
       "680397   mikeinsd77        \n",
       "90589    JAKAZiD           \n",
       "1167100  OzgeCann          \n",
       "...           ...          \n",
       "863898   dalispacheco      \n",
       "1581321  luvpups77         \n",
       "1359903  julie_bergmann    \n",
       "1484527  BrittCostanzo     \n",
       "437717   AQuietMadness     \n",
       "\n",
       "                                                                                                                      Text  \\\n",
       "470817   Barely awake. Back and feet ache                                                                                    \n",
       "1530967  Starts Intro to Digital Photography 2day!                                                                           \n",
       "680397   Dang I am so close yet so far from 1000 tweets                                                                      \n",
       "90589    @smitethis Really sorry I missed you last night, I accidentally slept through and missed the entire Soap Bubble.    \n",
       "1167100  @EneStramberg oh u r such a rud man boooooy !!  mickey... I want mickey mouse .. u got me ;))))                     \n",
       "...                                                                                                  ...                     \n",
       "863898   Finally I am free!!! the bimonthly stressful exams are over and I can sleep without anyone bothering me!!!!         \n",
       "1581321  YAY!!! i am done unpacking  room is neat and tidy                                                                   \n",
       "1359903  @TeamMileycyrusx I learned the guitar by myself, it's easy if you just get into it                                  \n",
       "1484527  Just a got Twitter                                                                                                  \n",
       "437717   @jordanknight The pg won't load for me.                                                                             \n",
       "\n",
       "                                                                                                           text_clean  \\\n",
       "470817   Barely awake. Back and feet ache                                                                               \n",
       "1530967  Starts Intro to Digital Photography day!                                                                       \n",
       "680397   Dang I am so close yet so far from  tweets                                                                     \n",
       "90589     Really sorry I missed you last night, I accidentally slept through and missed the entire Soap Bubble.         \n",
       "1167100   oh u r such a rud man boooooy !!  mickey... I want mickey mouse .. u got me ;))))                             \n",
       "...                                                                                     ...                             \n",
       "863898   Finally I am free!!! the bimonthly stressful exams are over and I can sleep without anyone bothering me!!!!    \n",
       "1581321  YAY!!! i am done unpacking  room is neat and tidy                                                              \n",
       "1359903   I learned the guitar by myself, it's easy if you just get into it                                             \n",
       "1484527  Just a got Twitter                                                                                             \n",
       "437717    The pg won't load for me.                                                                                     \n",
       "\n",
       "                                                                      text_normalized  \n",
       "470817   barely awake back feet ache                                                   \n",
       "1530967  starts intro digital photography day                                          \n",
       "680397   dang close yet far tweets                                                     \n",
       "90589    really sorry missed last night accidentally slept missed entire soap bubble   \n",
       "1167100  oh u r rud man boooooy mickey ... want mickey mouse .. u got ;))))            \n",
       "...                                                                     ...            \n",
       "863898   finally free ! bimonthly stressful exams sleep without anyone bothering !!!!  \n",
       "1581321  yay ! done unpacking room neat tidy                                           \n",
       "1359903  learned guitar easy get                                                       \n",
       "1484527  got twitter                                                                   \n",
       "437717   pg wont load                                                                  \n",
       "\n",
       "[3000 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_sm.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if(row[\"text_normalized\"] and len(str(row[\"text_normalized\"])) < 1000000):\n",
    "        doc = nlp(str(row[\"text_normalized\"]))\n",
    "        adjectives = []\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        lemmas = []\n",
    "\n",
    "        for token in doc:\n",
    "            lemmas.append(token.lemma_)\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "                nouns.append(token.lemma_)\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_)\n",
    "                \n",
    "        df_sm.at[i, \"text_lemma\"] = \" \".join(lemmas)                \n",
    "        df_sm.at[i, \"text_nouns\"] = \" \".join(nouns)\n",
    "        df_sm.at[i, \"text_adjectives\"] = \" \".join(adjectives)\n",
    "        df_sm.at[i, \"text_verbs\"] = \" \".join(verbs)\n",
    "        df_sm.at[i, \"text_nav\"] = \" \".join(nouns+adjectives+verbs)\n",
    "        df_sm.at[i, \"no_tokens\"] = len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sometimes, our cleaning reduces our text to nothing! Which makes a lot of stuff unable to run.\n",
    "\n",
    "## This hack has been helpful in that regard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in case we need it later:\n",
    "\n",
    "\n",
    "df_sm = df_sm[df_sm[\"text_nav\"].notnull()]\n",
    "df_sm.count()\n",
    "\n",
    "# shuffle the dataset for later.\n",
    "# df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "\n",
    "# Saving Cleaned Data to the Filesystem\n",
    "\n",
    "### So I can run without re-cleaning, or move a chunk to the Cloud for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned data\n",
    "\n",
    "df_sm.to_csv('cleaned01.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Cleaned Data from the Filesystem\n",
    "\n",
    "### Saved in various sizes, load as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sm = pd.read_csv(\"s140_cln_100k.csv\", encoding=\"utf-8\")\n",
    "\n",
    "## s140_cln_100k.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "# =========================\n",
    "# Data Exploration\n",
    "# =========================\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is dataset balanced?\n",
    "\n",
    "We know that the original dataset is split right down the middle, with 800,000 positive documents and 799,999 negative. Let's check that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Now, let's check that our hackishly sampled little subset is also balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Seems balanced enough for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show data types in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of numerical features\n",
    "\n",
    "Not the most useful thing, but helpful as a quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring text at different levels of cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm[['text_clean','text_normalized','text_lemma','text_nav']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib and seaborn and adjust some defaults\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a List of Tokens from a List of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    return text.split() if text != None else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df_sm.text_nav.map(my_tokenizer).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Frequencies with a Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(tokens)\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([t[0] for t in counter.most_common(200)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords from list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from a list of tokens.\"\"\"\n",
    "    return [t for t in tokens if t not in STOP_WORDS]\n",
    "\n",
    "# rebuild counter\n",
    "counter = Counter(remove_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of tuples into data frame\n",
    "freq_df = pd.DataFrame.from_records(counter.most_common(20),\n",
    "                                    columns=['token', 'count'])\n",
    "\n",
    "# create bar plot\n",
    "freq_df.plot(kind='bar', x='token');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud(counter):\n",
    "    \"\"\"A small wordloud wrapper\"\"\"\n",
    "    wc = WordCloud(width=1200, height=800, \n",
    "                   background_color=\"white\", \n",
    "                   max_words=200) \n",
    "    wc.generate_from_frequencies(counter)\n",
    "\n",
    "    # Plot\n",
    "    fig=plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Word Cloud!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive words, negative words\n",
    "\n",
    "Let's do some charts and clouds for exclusively positive or negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = df_sm[df_sm['sentiment']==0]\n",
    "neg_tokens = neg_df.text_nav.map(my_tokenizer).sum()\n",
    "neg_counter = Counter(neg_tokens)\n",
    "#neg_counter.most_common(20)\n",
    "neg_counter = Counter(remove_stopwords(neg_tokens))\n",
    "neg_freq_df = pd.DataFrame.from_records(neg_counter.most_common(20),\n",
    "                                    columns=['token', 'count'])\n",
    "\n",
    "# create bar plot\n",
    "neg_freq_df.plot(kind='bar', x='token');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(neg_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df_sm[df_sm['sentiment']==4]\n",
    "pos_tokens = pos_df.text_nav.map(my_tokenizer).sum()\n",
    "pos_counter = Counter(pos_tokens)\n",
    "#pos_counter.most_common(20)\n",
    "pos_counter = Counter(remove_stopwords(pos_tokens))\n",
    "pos_freq_df = pd.DataFrame.from_records(pos_counter.most_common(20),\n",
    "                                    columns=['token', 'count'])\n",
    "\n",
    "# create bar plot\n",
    "pos_freq_df.plot(kind='bar', x='token');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(pos_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, WOW, those are some mighty positive words, in the positive documents word cloud! \n",
    "\n",
    "The most frequently occurring words are clearly different in the text labeled as positive and the text labeled as negative.\n",
    "\n",
    "In the early phases of development, we were using a dataset, centered around the change at 800,000 rows, with 4000 samples. In this sample, the words \"Farrah\" and \"Fawcett\" were the most common words. This suggested perhaps that we need to be more careful about subsampling our data (perhaps a random selection approach would yeild less weird results?) it made for a surprise when we looked at positive and negative sentiment. Who would have thought that \"Farrah\" and \"Fawcett\" would be exclusively from statements with negative sentiment? Surprised me, for sure, but there it is. Data Exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring text complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm['no_tokens'] = df_sm.text_lemma\\\n",
    "  .map(lambda l: 0 if l==None else len(l.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean number of tokens by sentiment\n",
    "df_sm.groupby(['sentiment']) \\\n",
    "  .agg({'no_tokens':'mean'}) \\\n",
    "  .sort_values(by='no_tokens', ascending=False) \\\n",
    "  .plot(kind='bar', figsize=(7,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render plots as retina or png, because svg is very slow\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def multi_boxplot(data, x, y, ylim = None):\n",
    "    '''Wrapper for sns boxplot with cut-off functionality'''\n",
    "    # plt.figure(figsize=(30, 5))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xticks(rotation=90) \n",
    "\n",
    "    # order boxplots by median\n",
    "    ordered_values = data.groupby(x)[[y]] \\\n",
    "                         .median() \\\n",
    "                         .sort_values(y, ascending=False) \\\n",
    "                         .index\n",
    "        \n",
    "    sns.boxplot(x=x, y=y, data=data, palette='Set2', \n",
    "                order=ordered_values)\n",
    "\n",
    "    fig.set_size_inches(11, 6)\n",
    "    \n",
    "    # cut-off y-axis at value ylim\n",
    "    ax.set_ylim(0, ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_boxplot(df_sm, 'sentiment', 'no_tokens');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print text of outliers\n",
    "df_sm['text_lemma'][df_sm.no_tokens > 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut-off diagram at y=40\n",
    "\n",
    "# CAREFUL!!! this isn't that meaningful, and it takes for freaking ever to plot, even with only 4000 rows!!!\n",
    "\n",
    "# multi_boxplot(df_sm, 'username', 'no_tokens', ylim=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "\n",
    "# ===============================\n",
    "# Feature Engineering\n",
    "# ===============================\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering and Feature Selection are part of the Data Preparation stage of the CRISP-DM methodology. After data has been cleaned and explored, it must be transformed from raw, unstructured text into a structured numeric format that can be used as inputs for our models. Simpler Data Engineering techniques focus on vectorizing individual words, with little emphasis on the contexts of the words. We use Bag of Words and Bag of N-Grams to explore these simpler approaches. While easy to use, and not terribly demanding in terms of computer power required, these techniques are fundamentally less powerful than more modern, processor intensive techniques that concern themselves more with the context of the words. We use pre-trained word embeddings for our advanced feature engineering efforts, in order to avoid computational bottlenecks. If time allows, we may attempt to train our own embedding at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A vector space model is simply a mathematical model to represent unstructured text (or any other data) as numeric vectors, such that each dimension of the vector is a specific feature\\attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model\n",
    "\n",
    "The bag of words model represents each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0) or even weighted values. The model’s name is such because each document is represented literally as a ‘bag’ of its own words, disregarding word orders, sequences and grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(df_sm['text_nav'])\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab_bagowords = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab_bagowords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro dataframe visualization experiment\n",
    "\n",
    "To satisfy my curiosity, I'm going to make up a teeny weeny little dataframe, to see if I can see some values in one of these arrays as it gets previewed here in the pandas dataframe thing. We just see the corners. All zeroes. Let's see if it looks more satisfying with just a handful of rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_micro = df_sm[1:10]\n",
    "cv_micro = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_micro_matrix = cv_micro.fit_transform(df_micro['text_nav'])\n",
    "cv_micro_matrix = cv_micro_matrix.toarray()\n",
    "cv_micro_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab_micro = cv_micro.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_micro_matrix, columns=vocab_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oh, that's much more gratifying!\n",
    "\n",
    "While it's clearly useless from a practical perspective, it's nice to be able to see some numbers that aren't zeroes and let me know that the code is doing the thing I expect the code to be doing. I've got to try this on the next ones, too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-Grams model\n",
    "\n",
    "A word is just a single token, often known as a unigram or 1-gram. We already know that the Bag of Words model doesn’t consider order of words. But what if we also wanted to take into account phrases or collection of words which occur in a sequence? N-grams help us achieve that. An N-gram is basically a collection of word tokens from a text document such that these tokens are contiguous and occur in a sequence. Bi-grams indicate n-grams of order 2 (two words), Tri-grams indicate n-grams of order 3 (three words), and so on. The Bag of N-Grams model is hence just an extension of the Bag of Words model so we can also leverage N-gram based features. The following example depicts bi-gram based features in each document feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
    "bv = CountVectorizer(ngram_range=(2,5))\n",
    "bv_matrix = bv.fit_transform(df_sm['text_nav'])\n",
    "\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab_ngrams = bv.get_feature_names()\n",
    "pd.DataFrame(bv_matrix, columns=vocab_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us feature vectors for our documents, where each feature consists of a bi-gram representing a sequence of two words and values represent how many times the bi-gram was present for our documents.\n",
    "\n",
    "### And now with the Micro Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_micro = CountVectorizer(ngram_range=(2,2))\n",
    "bv_micro_matrix = bv_micro.fit_transform(df_micro['text_nav'])\n",
    "\n",
    "bv_micro_matrix = bv_micro_matrix.toarray()\n",
    "vocab_micro = bv_micro.get_feature_names()\n",
    "pd.DataFrame(bv_micro_matrix, columns=vocab_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Model\n",
    "\n",
    "There are some potential problems which might arise with the Bag of Words model when it is used on large corpora. Since the feature vectors are based on absolute term frequencies, there might be some terms which occur frequently across all documents and these may tend to overshadow other terms in the feature set. The TF-IDF model tries to combat this issue by using a scaling or normalizing factor in its computation. TF-IDF stands for Term Frequency-Inverse Document Frequency. There are multiple variants of this model but they all end up giving quite similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix = tv.fit_transform(df_sm['text_nav'])\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab_tfidf = tv.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotta try that with the Micro Dataframe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_micro = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_micro_matrix = tv_micro.fit_transform(df_micro['text_nav'])\n",
    "tv_micro_matrix = tv_micro_matrix.toarray()\n",
    "\n",
    "vocab_micro = tv_micro.get_feature_names()\n",
    "pd.DataFrame(np.round(tv_micro_matrix, 2), columns=vocab_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "\n",
    "# ===========================\n",
    "# Feature Selection\n",
    "# ===========================\n",
    "\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection is an essential step, which allows us to identify which features are most important to the predictive abilities of our models. Filter methods of Feature Selection involve looking at individual features in isolation, giving them a score by which they can be ranked in terms of their usefulness. We use Univariate Chi-squared statistical tests. Wrapper methods of Feature Selection consider sets of features in combination, which can give deeper insights into which features to select given their interactions and correlations with one another. We use Recursive Feature Elimination and Bagged Decision Trees for this type of feature selection. There are also Embedded Feature Selection techniques, however, these are done in concert with the modeling phase of the project, and if they will be attempted, they will be attempted during the modeling phase of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Method\n",
    "\n",
    "### The scikit-learn library provides the SelectKBest class that uses the chi squared (chi^2) statistical test to select the best features\n",
    "\n",
    "Our Bag of Words vectorization has provided us with over a thousand features. SelectKBest can identify which of these features are most strongly correlated with our sentiment label. We produce a new Bag of Words containing only the features SelectKBest determines are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# select the 500 features that have the strongest correlation to a class from the\n",
    "# original thousands of features\n",
    "selector = SelectKBest(chi2, k=500)\n",
    "selected_features = \\\n",
    "bow_selected = selector.fit(cv_matrix, df_sm['sentiment']).get_support(indices=True)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Selected Features\n",
    "\n",
    "That array of numbers tells us which indecies of the Bag of Words vectorization were deemed most important by SelectKBest. Kind of boring on its own. It would be much more gratifying to see the actual words it has decided are most important.\n",
    "\n",
    "## View list of words selected by SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in selected_features:\n",
    "    print(vocab_bagowords[x], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Method\n",
    "\n",
    "### Recursive Feature Elimination\n",
    "\n",
    "The Recursive Feature Elimination (or RFE) method works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = LogisticRegression(solver='lbfgs', max_iter=6)\n",
    "rfe = RFE(model, 500)\n",
    "fit = rfe.fit(cv_matrix, df_sm['sentiment'])\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=0\n",
    "for x in fit.support_:\n",
    "    if x:\n",
    "        print(vocab_bagowords[i], end=' ')\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Wrapper Methods\n",
    "\n",
    "## Bagged Decision Trees - ExtraTreesClassifier classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "model_bagged = ExtraTreesClassifier(n_estimators=10)\n",
    "model_bagged.fit(cv_matrix, df_sm['sentiment'])\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "print(model_bagged.feature_importances_)\n",
    "# np.set_printoptions(threshold=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "n=0\n",
    "min_imp = 0.00041\n",
    "min_starred = 0.004\n",
    "for x in model_bagged.feature_importances_:\n",
    "    if x>min_imp:\n",
    "        if x>min_starred:\n",
    "            print('***', end='')\n",
    "        print(vocab_bagowords[i], end=' ')\n",
    "        n=n+1\n",
    "    i=i+1\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Number of Features with minimum importance ', min_imp, ': ', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that the stars ( *** ) denote features that were given much higher importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection on TF-IDF encoded features\n",
    "\n",
    "### Filter Method - SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_tfidf = SelectKBest(chi2, k=500)\n",
    "selected_features_tfidf = \\\n",
    "selector_tfidf.fit(tv_matrix, df_sm['sentiment']).get_support(indices=True)\n",
    "selected_features_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in selected_features_tfidf:\n",
    "    print(vocab_tfidf[x], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE on TF-IDF vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_tfidf = LogisticRegression(solver='lbfgs', max_iter=6)\n",
    "#rfe = RFE(model_tfidf, 500)\n",
    "#fit_tfidf = rfe.fit(tv_matrix, df_sm['sentiment'])\n",
    "#print(\"Num Features: %d\" % fit.n_features_)\n",
    "#print(\"Selected Features: %s\" % fit.support_)\n",
    "#print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=0\n",
    "for x in fit_tfidf.support_:\n",
    "    if x:\n",
    "        print(vocab_bagowords[i], end=' ')\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting TF-IDF Features with Bagged Decision Trees\n",
    "\n",
    "### This represents our most sophisticated feature selection method, being used with our most sophisticated feature engineering method.\n",
    "\n",
    "Earlier, in the Feature Engineering section, we vectorized our text data using the Term Frequency - Inverse Document Frequency method. This resulted in our documents being represented as vectors with thousands of dimensions. Here we will select the most statistically relevant dimensions using a Wrapper Method known as an Extra Trees Classifier, which is an example of Bagged Decision Trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bagged_tfidf = ExtraTreesClassifier(n_estimators=20)\n",
    "model_bagged_tfidf.fit(tv_matrix, df_sm['sentiment'])\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "print(model_bagged_tfidf.feature_importances_)\n",
    "# np.set_printoptions(threshold=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making our new array containing only selected features.\n",
    "\n",
    "### And saving back a copy of the unselected array for future comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the unselected features for benchmarking later.\n",
    "tv_matrix_unselected = tv_matrix\n",
    "tv_matrix_selected = np.array([])\n",
    "i=0\n",
    "n=0\n",
    "min_imp = 0.00005\n",
    "min_starred = 0.003\n",
    "#vector_temp = np.array([np.size(tv_matrix, 0)])\n",
    "#vector_temp.shape = (np.size(tv_matrix, 0),1)\n",
    "# a = numpy.zeros(shape=(5,2))\n",
    "vector_temp = np.zeros(shape = ((np.size(tv_matrix, 0)), 1))\n",
    "np.shape(vector_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_temp=tv_matrix[:,i]\n",
    "tv_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tv_matrix[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numrows = (np.size(tv_matrix, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in model_bagged_tfidf.feature_importances_:\n",
    "    \n",
    "    if x>min_imp:\n",
    "        tv_temp=tv_matrix[:,i]\n",
    "        tv_temp.shape = (numrows, 1)\n",
    "        vector_temp=np.append(vector_temp, tv_temp, axis = 1)\n",
    "        if x>min_starred:\n",
    "            \n",
    "            print('***', end='')\n",
    "        print(vocab_tfidf[i], end=' ')\n",
    "        n=n+1\n",
    "    i=i+1\n",
    "    #vector_temp = np.array([])\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Number of Features with minimum importance ', min_imp, ': ', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(tv_matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(vector_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_temp[0,:])\n",
    "vector_temp = vector_temp[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tv_matrix = vector_temp\n",
    "vector_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_matrix = vector_temp\n",
    "tv_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A record and summary of 6 types of Feature Selection\n",
    "\n",
    "## Because I WILL be dropping RFE for being too intensive.\n",
    "\n",
    "### Running with 3000 samples gave us vectors with around 5000 dimensions. After much running time, we were able to visually inspect the 500 most important features, as selected by each method. It is perhaps not informative to look at, and compare, these lists of features - but it is fun. And it may give some \"subconscious\" intuition into that age-old question, \"What are these things thinking?\" So, before I disable some of these, so I can run bigger datasets through all this code, here's a record of this activity, in the comments/markdown.\n",
    "\n",
    "I will, quite simple, strip the top two and bottom two rows of each output, and paste them here with clear labels. Pretty sure I won't be running RFE going forward - spins forever, with a pretty low number of samples.\n",
    "\n",
    "#### SelectKBest with Bag of Words\n",
    "account actress add age agree ahhh airport amazing american angel animal anne anniversary announce answer appreciate arm arrive attend awesome awful aww awww back bad battle beach beautiful bed bedtime beer believe beloved beyonce birthday blog blonde blue boo book boy boyfriend brand brave britney bro ... tomorrow tongue toooo tragic trust tt tuesday tweet twilight twin twit twitter ugh ughhh unc unfair update updates upgrade upload upset ur use vacuum vegas version voice vote wait walters want warped wat watch way wayne website welcome window wish woke woman wonderful woop work wrist wrong wtf xd xoxo yay young youth yup\n",
    "#### RFE with Bag of Words\n",
    "aargh account add addict address afraid age agree ahhh air airport album amazing angel anniversary answer app arm arrive artistic attack attend attention awesome awful awsome aww baby back bad bag battle bc beach bed bet big birthday bit blib blog blue bonjour boo bore bored boy boyfriend brain brand break ... tragic true trust tuesday tv tweeting twin twit twitter ugh unc understand unfair updates upgrade upload upset vacation vegas version vibe video vote wait walmart wanna want wat watch way wayne welcome whole whyyyy wife win wish woke wonder wonderful woop word work working world write wrong wtf xavier xd yay young yup\n",
    "#### Extra Trees Forest with Bag of Words\n",
    "able account add addict afraid age agree ahhhh air album alex amazing angel apettite arm art audrey awesome awful aww baby back bad bag battle bc be beach beautiful bed begin beloved big bit blaaaaarg blache blog boo bore boy boyfriend break bubba bug burn bus bye call can cancel cancer car care cat cause ... twitter ugh ughh uncle unfair unusual upgrade upgradeable upload upset upsetting ur use vacation vibe vote wait wake walk wanna want warm warning watch way weather wee week weekend welcome well wholee will wish wishing woke woman wonderful work working world would wrong xoxo yay yeanot year yesterday young yup zoo\n",
    "#### SelectKBest with TF-IDF\n",
    "account actress add adore age agree ahaha ahhh airport amazing american anal angel anne anniversary announce annoying answer apologize appreciate arm arrive attend awesome awful awsome aww back background bad bag battle beach beautiful beauty bed bedtime beer beie believe beloved bestie beyonce birthday ... touchy tragic trust tuesday tweetie twilight twin twit twitt twitter ugh ughhh unc uncle understand unfair updates upload upset upsetting use va vegas version view voice vote wait wallpaper want watch way website welcome westney window wish woke woman wonderful woo work wrong xd xoxo yay yayyy young youth yup zoo \n",
    "#### RFE with TF-IDF\n",
    "able account add address afraid age agree ahhh airport album amazing american angel anne anniversary answer appreciate arm arrive art attention awesome awful awsome aww back background bad bag battle bc be beach bed believe big birthday bit blog blue bonjour boo book bore bout boy boyfriend brand brb break bro ... upsetting va vacation vegas version vibe vote wait walk wanna want watch way wee welcome whole window wish wishing woke wonder wonderful word work working world wrong wtf xoxo yay young yud yup zoo\n",
    "#### Extra Trees Forest with TF-IDF\n",
    "able actress add addict afraid age agree ahhhh airport alex amazing andrewcilley angel apettite arm art awesome awful aww baby back bad battle bc be beach beautiful bed believe beloved big bit blaaaaarg blache blackberrys blog boo book bore bowls boy boyfriend break brussels bubba bug bunny burn butterfly ... upgrade upgradeable upload upset upsetting ur use vacation vegas vibe video wait wake walk walmart wanna want warm warning watch way weather wee week weekend welcome well whole wholee will window wish wishing woke woman wonderful word work working world worried would wrong xoxo yay yeanot year yesterday young yud yup yuri zoo  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last but (maybe) not least - Selecting with our Bag-of-N-Grams\n",
    "\n",
    "#### Comparing features for Bag-o-words and TF-IDF was very much like comparing apples with apples.\n",
    "\n",
    "This is going to look a little different, I'll bet, so I'll keep it out from between those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 50 features that have the strongest correlation to a class from the\n",
    "# original thousands of features\n",
    "selector_ngrams = SelectKBest(chi2, k=400)\n",
    "selected_features_ngrams = \\\n",
    "ngrams_selected = selector_ngrams.fit(bv_matrix, df_sm['sentiment']).get_support(indices=True)\n",
    "selected_features_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in selected_features_ngrams:\n",
    "    print(vocab_ngrams[x], end=' -       - ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE with Bag of N-Grams?\n",
    "\n",
    "## Sorry, no.\n",
    "\n",
    "## RFE takes a fabulously long time, even with a few thousand vectors with a few thousand dimensions.\n",
    "\n",
    "My bag-o-n-grams has 2-grams, 3-grams, and 4-grams. It's kinda fun. With 3000 samples, it has TENS of THOUSANDS of dimensions. I think RFE, in general, won't play a big role in my plans. I suppose I could run this on a few hundred samples to see if it works. But so what if it does? Can't move ahead with any constructive with dinky little dataframes like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do the forest with the ngrams and call it a day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_bagged_ngrams = ExtraTreesClassifier(n_estimators=20)\n",
    "model_bagged_ngrams.fit(bv_matrix, df_sm['sentiment'])\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "print(model_bagged_ngrams.feature_importances_)\n",
    "# np.set_printoptions(threshold=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=0\n",
    "n=0\n",
    "min_imp = 0.00041\n",
    "min_hilite = 0.0028\n",
    "for x in model_bagged_ngrams.feature_importances_:\n",
    "    if x>min_imp:\n",
    "        if x>min_hilite:\n",
    "            print('|||--', end=\"\")\n",
    "            print(vocab_ngrams[i], end='--||| -       - ')\n",
    "        else:\n",
    "            print(vocab_ngrams[i], end=' -       - ')\n",
    "        n=n+1\n",
    "    i=i+1\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Number of Features with minimum importance ', min_imp, ': ', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1 - Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ------> CUT\n",
    "\n",
    "Code, output, and markdown for everything up to Milestone 1 has been removed for ease of Milestone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning of Milestone 2\n",
    "\n",
    "Working models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Selection\n",
    "\n",
    "### In this section we will train and test models using 6 different algorithms, and then compare their accuracy.\n",
    "\n",
    "This can help us understand which type of models works best with our data, and allow us to identify problems using certain types of models with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sm['text_nav'], df_sm['sentiment'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(count_vect.transform([\"AArgh, this is the silliest thing ever\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(count_vect.transform([\"I absolutely love this code.\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(count_vect.transform([\"Not sure what I think of this one.\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(count_vect.transform([\"Why would you do that?\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "models = [\n",
    "    DecisionTreeClassifier(criterion='entropy', max_depth=2),\n",
    "    KNeighborsClassifier(n_neighbors=1),\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "    \n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, tv_matrix, df_sm['sentiment'], scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(rc={'figure.figsize':(14,6)})\n",
    "\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df, width=0.8).set_title('Comparing Accuracies of Different Model Types')\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=12, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.figure(figsize=(16, 6))\n",
    "#plt.set_size_inches(18.5, 10.5)\n",
    "plt.savefig('test2png.png', dpi=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "\n",
    "My first attempts at Ensembling yielded pretty dubious results. More dubious than some of these standalone models. Looking at this graphs helps explain why - the code I was originally using for Ensemble Methods used Decision Tree and K-Nearest-Neighbour. This graph suggests that these models perform poorly compared to SVC, NB, etc. Which gives us something to go on, in our quest to make the Ensemble Methods produce better results - Ensemble more powerful models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================\n",
    "\n",
    "# LinearSVC (<i>Support Vector Classifier</i>) Model\n",
    "\n",
    "Split into training and test sets, train model on training set, generate predictions on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model_svc = LinearSVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(tv_matrix, df_sm['sentiment'], df_sm.index, test_size=0.33, random_state=0)\n",
    "model_svc.fit(X_train, y_train)\n",
    "y_pred = model_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix: LinearSVC\n",
    "\n",
    "Calculate, and display, true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "We'll show both the raw matrix, and a \"Heatmap\", which looks cool, but doesn't give too much insight with only two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('RAW CONFUSION MATRIX', '\\n')\n",
    "print(conf_mat, '\\n')\n",
    "print('Because the heatmap may LOOK cool, but doesn\\'t tell us much, with only two labels \\n')\n",
    "print('--------------------\\n')\n",
    "print('Heatmap of Consfusion Matrix:')\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['pos','neg'], yticklabels=['pos','neg'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svc.fit(tv_matrix, df_sm['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and f1-Score: LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=['0','4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve: LinearSVC\n",
    "\n",
    "Using code and wisdom from https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n",
    "A useful tool when predicting the probability of a binary outcome is the <b>Receiver Operating Characteristic curve</b>, or ROC curve.\n",
    "\n",
    "It is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve and auc\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Shrink back our Seaborn graph size - don't need it as big as we had for the accuracy comparison.\n",
    "sns.set(rc={'figure.figsize':(10,5)})\n",
    "\n",
    "# generate 2 class dataset\n",
    "#X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "X = tv_matrix\n",
    "y = df_sm['sentiment']\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# We use our model as defined above\n",
    "model_svc.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "# use _predict_proba_lr(testX) with SVC\n",
    "lr_probs = model_svc._predict_proba_lr(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('LinearSVC: ROC AUC=%.3f' % (lr_auc))\n",
    "print('\\nROC Curve for LinearSVC Classifier:')\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=4)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs, pos_label=4)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='LinearSVC')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.title('ROC Curve for LinearSVC Classifier')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ROC curve shows considerably higher \"skill\" than the \"no skill\" curve, which represents the perfomance of a classifier that just guesses the most likely thing.\n",
    "\n",
    "# AUC - Area Under the Curve\n",
    "\n",
    "The \"skill\" of this model can be represented by the AUC score - the Area Under the Curve - and we do note that despite the LinearSVC model having the highest accuracy and f1-score of the models we tried, Logistic Regression appears to have a higher AUC, as we will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve: LinearSVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curve and f1\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# use our model\n",
    "model = model_svc\n",
    "\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model._predict_proba_lr(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = model.predict(testX)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(testy, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('LinearSVC: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(testy[testy==1]) / len(testy)\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='LinearSVC')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.title('Precision-Recall curve for LinearSVC Classifier')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision-recall curve plot show the precision/recall for each threshold for a Linear Support Vector Classifier model (orange) compared to a no skill model (blue).\n",
    "\n",
    "## When to Use ROC vs. Precision-Recall Curves?\n",
    "\n",
    "Again from https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "\n",
    "Generally, the use of ROC curves and precision-recall curves are as follows:\n",
    "\n",
    "- ROC curves should be used when there are roughly equal numbers of observations for each class.\n",
    "- Precision-Recall curves should be used when there is a moderate to large class imbalance.\n",
    "\n",
    "The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\n",
    "\n",
    "Some go further and suggest that using a ROC curve with an imbalanced dataset might be deceptive and lead to incorrect interpretations of the model skill.\n",
    "\n",
    "The main reason for this optimistic picture is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\n",
    "\n",
    "## As our dataset is very very balanced, we will lean towards ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves: LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_learning_curves\n",
    "#plot_learning_curves(X_train, y_train, X_test, y_test, model_svc)\n",
    "plot_learning_curves(trainX, trainy, testX, testy, model_svc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "We'll do the same metrics for Logistic Regression as we did for LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(random_state=0)\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_pred = model_lr.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('\\nConfusion Matrix for Logistic Regression Model: \\n')\n",
    "print(conf_mat , '\\n')\n",
    "print('Confusion Matrix for Logistic Regression Model as a little heatmap:')\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['pos','neg'], yticklabels=['pos','neg'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print('\\nPrecision, Recall, F1-scores for Logistic Regression Model:', '\\n')\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=['0','4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve: Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our vectorized and feature-selected data\n",
    "X = tv_matrix\n",
    "y = df_sm['sentiment']\n",
    "\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# use our model as defined above\n",
    "model = model_lr\n",
    "\n",
    "#LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "print('\\nROC curve for Logistic Regression Classifier:')\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=4)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs, pos_label=4)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.title('ROC Curve for Logistic Regression Classifier')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# fit a model\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# use our model\n",
    "model = model_lr\n",
    "\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = model.predict(testX)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(testy, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic Regression: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(testy[testy==1]) / len(testy)\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic Regression')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "pyplot.title('Precision-Recall curve for Logistic Regression Classifier')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_learning_curves\n",
    "#plot_learning_curves(X_train, y_train, X_test, y_test, model_svc)\n",
    "plot_learning_curves(trainX, trainy, testX, testy, model_lr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================\n",
    "\n",
    "# Multinomial NB Classifier\n",
    "\n",
    "The <b>Multinomial Naive Bayes classifier</b> is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mnb = MultinomialNB()\n",
    "model_mnb.fit(X_train, y_train)\n",
    "y_pred = model_mnb.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('\\nConfusion Matrix for Multinomial Naive Bayes Classifier: \\n')\n",
    "print(conf_mat , '\\n')\n",
    "print('Confusion Matrix for Multinomial Naive Bayes Classifier as a little heatmap:')\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['pos','neg'], yticklabels=['pos','neg'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print('\\nPrecision, Recall, F1-scores for Multinomial Naive Bayes Classifier:', '\\n')\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=['0','4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC and AUC: MultinomialNB\n",
    "\n",
    "As with above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our vectorized and feature-selected data\n",
    "X = tv_matrix\n",
    "y = df_sm['sentiment']\n",
    "\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# use our model as defined above\n",
    "model = model_mnb\n",
    "\n",
    "#LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('MultinomialNB: ROC AUC=%.3f' % (lr_auc))\n",
    "print('\\nROC Curve for MultinomialNB:')\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=4)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs, pos_label=4)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='MultinomialNB')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves: MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_learning_curves(trainX, trainy, testX, testy, model_mnb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===============================================\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "It did perform poorly on the \"accuracy\" comparison, especially with the really small dataset, but as we've seen above, accuracy is a poor predictor of other performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfc = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=0)\n",
    "#model_mnb = MultinomialNB()\n",
    "model_rfc.fit(X_train, y_train)\n",
    "y_pred = model_rfc.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('\\nConfusion Matrix for Random Forest Classifier: \\n')\n",
    "print(conf_mat , '\\n')\n",
    "print('Confusion Matrix for Random Forest Classifier as a little heatmap:')\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['pos','neg'], yticklabels=['pos','neg'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print('\\nPrecision, Recall, F1-scores for Random Forest Classifier Classifier:', '\\n')\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=['0','4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! That is our worst one so far, both in terms of accuracy AND f1-score. Let's do the ROC curve!\n",
    "\n",
    "## ROC Curve and AUC metric: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our vectorized and feature-selected data\n",
    "X = tv_matrix\n",
    "y = df_sm['sentiment']\n",
    "\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "\n",
    "# use our model as defined above\n",
    "model = model_rfc\n",
    "\n",
    "#LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Random Forest Classifier: ROC AUC=%.3f' % (lr_auc))\n",
    "print('\\nROC Curve for Random Forest Classifier:')\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=4)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs, pos_label=4)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Random Forest Classifier')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_learning_curves\n",
    "#plot_learning_curves(X_train, y_train, X_test, y_test, model_svc)\n",
    "plot_learning_curves(trainX, trainy, testX, testy, model_rfc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting points of comparison!!\n",
    "\n",
    "Early in this project, I was doing sentiment predictions with Afinn, to be used as a baseline later. At our first arrival at this point, things are looking better already!\n",
    "\n",
    "#### Weighted Avg F1-Scores\n",
    "-----------------------------------\n",
    "\n",
    "0.63 - Afinn original values.\n",
    "\n",
    "0.70 - Logistic Regression, 800 rows total, Tf-idf encoding\n",
    "\n",
    "0.74 - LinearSVC, 800 rows total, Tf-idf encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================\n",
    "\n",
    "# Ensembling\n",
    "\n",
    "# ============================\n",
    "\n",
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking). Ensemble methods can be divided into two groups: sequential ensemble methods where the base learners are generated sequentially (e.g. AdaBoost) and parallel ensemble methods where the base learners are generated in parallel (e.g. Random Forest). The basic motivation of sequential methods is to exploit the dependence between the base learners since the overall performance can be boosted by weighing previously mislabeled examples with higher weight. The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging.\n",
    "\n",
    "Most ensemble methods use a single base learning algorithm to produce homogeneous base learners, i.e. learners of the same type leading to homogeneous ensembles. There are also some methods that use heterogeneous learners, i.e. learners of different types, leading to heterogeneous ensembles. In order for ensemble methods to be more accurate than any of its individual members the base learners have to be as accurate as possible and as diverse as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Bagging stands for <b>bootstrap aggregation</b>. One way to reduce the variance of an estimate is to average together multiple estimates. For example, we can train $M$ different trees $f_m$ on different subsets of the data (chosen randomly with replacement) and compute the ensemble:\n",
    "\n",
    "\\begin{equation}\n",
    "   f(x) = \\frac{1}{M}\\sum_{m=1}^{M}f_m(x) \n",
    "\\end{equation}\n",
    "\n",
    "<b>Translation:</b> <i><u>The final result is the average of the M sub-results.</u></i>\n",
    "\n",
    "Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses voting for classification and averaging for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the coding example used for this section of the project, ensemble methods are used on Decision Tree Classifiers and K Nearest Neighbour Classifiers.\n",
    "\n",
    "As we saw in our comparison on standalone models, these are the two worst, of the six we tried. We will instead use Logistic Regression and a Linear Support Vector Classifier for our Ensemble Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = iris.data[:, 0:2], iris.target\n",
    "\n",
    "X = tv_matrix\n",
    "y = np.array(df_sm['sentiment'])\n",
    "    \n",
    "clf1 = LogisticRegression(random_state=0)\n",
    "clf2 = LinearSVC()   \n",
    "\n",
    "bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=12, max_samples=0.8, max_features=0.8)\n",
    "bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=12, max_samples=0.8, max_features=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "label = ['Logistic Regression', 'LinearSVC', 'Bagging Regression', 'Bagging SVC']\n",
    "clf_list = [clf1, clf2, bagging1, bagging2]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    \n",
    "    \n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "        \n",
    "    #clf.fit(X, y)\n",
    "    \n",
    "    pca = PCA(n_components = 2)\n",
    "    X_flattened = pca.fit_transform(X)\n",
    "    #clf.fit(X_flattened, y)\n",
    "    #clf.fit(X, y)\n",
    "    clf.fit(X_flattened, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X_flattened, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "    \n",
    "\n",
    "    #plot_decision_regions(X_train2, y_train, clf=clf, legend=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much better accuracy with these algorithms! Moved us from the low 60s to the mid 70s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when we went back and chose more features, and ran everything with more rows of data, we find that we are able to get similar scores for the ensembled models as we did for the standalones. This was an improvement!\n",
    "\n",
    "# Bagging with Logistic Regression\n",
    "\n",
    "## Look for ideal ensembling size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble Size\n",
    "# num_est = map(int, np.linspace(1,100,20))\n",
    "num_est = np.linspace(1,50,20).astype(int)\n",
    "bg_clf_cv_mean = []\n",
    "bg_clf_cv_std = []\n",
    "for n_est in num_est:    \n",
    "    bg_clf = BaggingClassifier(base_estimator=clf1, n_estimators=n_est, max_samples=0.8, max_features=0.8)\n",
    "    scores = cross_val_score(bg_clf, X, y, cv=3, scoring='accuracy')\n",
    "    bg_clf_cv_mean.append(scores.mean())\n",
    "    bg_clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "# caps = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Ensemble Size'); plt.title('Bagging Tree Ensemble with Regression Classifiers');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We started using 10 in the ensemble, based on how this graph keeps turning out we increased that to 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves for Bagging with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, bagging1, print_model=False, style='ggplot')\n",
    "plt.title('Learning Curves for Bagging with Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows learning curves for the bagging tree ensemble. We can see an average error of  0.35  on the training data and a U-shaped error curve for the testing data. The smallest gap between training and test errors occurs at around  80%  of the training set size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix: Bagging with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bagging1.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('RAW CONFUSION MATRIX', '\\n')\n",
    "print(conf_mat, '\\n')\n",
    "print('--------------------\\n')\n",
    "print('Heatmap of Consfusion Matrix:')\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['pos','neg'], yticklabels=['pos','neg'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, and F1-Score for Bagging with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=['0','4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve for Bagging with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "#X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "X = tv_matrix\n",
    "y = df_sm['sentiment']\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# We use our model as defined above\n",
    "bagging1.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "# use _predict_proba_lr(testX) with SVC\n",
    "lr_probs = bagging1.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('LinearSVC: ROC AUC=%.3f' % (lr_auc))\n",
    "print('\\nROC Curve for Bagging Ensemble with Logistic Regression Classifier:')\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=4)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs, pos_label=4)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Bagging Ensemble with Logistic Regression')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.title('ROC Curve for Bagging Ensemble with Logistic Regression Classifier')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hmm, not so thrilling?\n",
    "\n",
    "In one run, we got 0.747 here, compared with 0.750 for straight Logistic Regression. Losing performance with ensemble methods? At least not losing as much as we were earlier..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging with Linear SVC\n",
    "\n",
    "### As above, but with Ensembles of Linear SVC classifiers rather than Ensembles of Logistic Regression classifiers.\n",
    "\n",
    "## Find optimal Ensemble size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_est = np.linspace(1,50,20).astype(int)\n",
    "bg_clf_cv_mean = []\n",
    "bg_clf_cv_std = []\n",
    "for n_est in num_est:    \n",
    "    bg_clf = BaggingClassifier(base_estimator=clf2, n_estimators=n_est, max_samples=0.8, max_features=0.8)\n",
    "    scores = cross_val_score(bg_clf, X, y, cv=3, scoring='accuracy')\n",
    "    bg_clf_cv_mean.append(scores.mean())\n",
    "    bg_clf_cv_std.append(scores.std())\n",
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "# caps = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Ensemble Size'); plt.title('Bagging Tree Ensemble with LinearSVC Classifiers');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve for Bagging Tree Ensemble with SVC Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, bagging2, print_model=False, style='ggplot')\n",
    "plt.title('Learning Curves for Bagging with LinearSVC Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Bagging with Linear SVC\n",
    "\n",
    "#### Confusion Matrix, Precision, Recall, and F1-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bagging2.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('RAW CONFUSION MATRIX', '\\n')\n",
    "print(conf_mat, '\\n')\n",
    "print('--------------------\\n')\n",
    "print('Heatmap of Consfusion Matrix:')\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['pos','neg'], yticklabels=['pos','neg'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=['0','4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve for Bagging Ensemble with Linear SVC Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "#X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "X = tv_matrix\n",
    "y = df_sm['sentiment']\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# We use our model as defined above\n",
    "bagging2.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "# use _predict_proba_lr(testX) with SVC\n",
    "lr_probs = bagging2.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('LinearSVC: ROC AUC=%.3f' % (lr_auc))\n",
    "print('\\nROC Curve for Bagging Ensemble with Linear SVC Classifiers:')\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=4)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs, pos_label=4)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Bagging Ensemble with Linear SVC Classifiers')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.title('ROC Curve for Bagging Ensemble with Linear SVC Classifiers')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "# Boosting\n",
    "\n",
    "Boosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners (models that are only slightly better than random guessing, such as small decision trees) to weighted versions of the data, where more weight is given to examples that were mis-classified by earlier rounds. The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. The principal difference between boosting and the committee methods such as bagging is that base learners are trained in sequence on a weighted version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tv_matrix\n",
    "y = np.array(df_sm['sentiment'])\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "num_est = [1, 5, 25, 100]\n",
    "label = ['AdaBoost (n_est=1)', 'AdaBoost (n_est=5)', 'AdaBoost (n_est=25)', 'AdaBoost (n_est=100)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for n_est, label, grd in zip(num_est, label, grid):     \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    \n",
    "    \n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)\n",
    "    pca = PCA(n_components = 2)\n",
    "    X_flattened = pca.fit_transform(X)\n",
    "    boosting.fit(X_flattened, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X_flattened, y=y, clf=boosting, legend=2)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaBoost algorithm is illustrated in the figure above. Each base learner consists of a decision tree with depth $1$, thus classifying the data based on a feature threshold that partitions the space into two regions separated by a linear decision surface that is parallel to one of the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Size for AdaBoost with Regression\n",
    "\n",
    "It appears that a higher number of classifiers in the Ensemble helps - hundreds or more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble Size\n",
    "#num_est = map(int, np.linspace(1,100,20))\n",
    "num_est = np.linspace(1,200,20).astype(int)\n",
    "bg_clf_cv_mean = []\n",
    "bg_clf_cv_std = []\n",
    "for n_est in num_est:\n",
    "    ada_clf = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)\n",
    "    scores = cross_val_score(ada_clf, X, y, cv=3, scoring='accuracy')\n",
    "    bg_clf_cv_mean.append(scores.mean())\n",
    "    bg_clf_cv_std.append(scores.std())\n",
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Ensemble Size'); plt.title('AdaBoost Ensemble');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=100)\n",
    "        \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, boosting, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix, F1-Score, etc for Boosting with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = boosting.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('RAW CONFUSION MATRIX', '\\n')\n",
    "print(conf_mat, '\\n')\n",
    "print('--------------------\\n')\n",
    "print('Heatmap of Consfusion Matrix:')\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['pos','neg'], yticklabels=['pos','neg'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=['0','4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve for Boosting with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "#X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "X = tv_matrix\n",
    "y = df_sm['sentiment']\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# We use our model as defined above\n",
    "boosting.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "# use _predict_proba_lr(testX) with SVC\n",
    "lr_probs = boosting.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('LinearSVC: ROC AUC=%.3f' % (lr_auc))\n",
    "print('\\nROC Curve for Boosting with Regression Classifiers:')\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=4)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs, pos_label=4)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Boosting with Regression Classifiers')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.title('ROC Curve for Boosting with Regression Classifiers')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "# Stacking\n",
    "\n",
    "Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on complete training set then the meta-model is trained on the outputs of base level model as features. The base level often consists of different learning algorithms and therefore stacking ensembles are often heterogeneous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tv_matrix\n",
    "y = np.array(df_sm['sentiment'])\n",
    "\n",
    "clf1 = LogisticRegression(random_state=0)\n",
    "clf2 = LogisticRegression(random_state=0)\n",
    "clf3 = LogisticRegression(random_state=0)\n",
    "clf4 = LinearSVC()\n",
    "clf5 = LinearSVC()\n",
    "clf6 = LinearSVC()\n",
    "#clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "#clf2 = RandomForestClassifier(random_state=1)\n",
    "#clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3, clf4, clf5, clf6], \n",
    "                          meta_classifier=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Logistic Regression', 'Logistic Regression', 'Logistic Regression', 'LinearSVC', 'LinearSVC', 'LinearSVC', 'Stacking Classifier']\n",
    "clf_list = [clf1, clf2, clf3, clf4, clf5, clf6, sclf]\n",
    "    \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "for clf, label, grd in zip(clf_list, label, grid):\n",
    "    \n",
    "    pca = PCA(n_components = 2)\n",
    "    X_flattened = pca.fit_transform(X)\n",
    "        \n",
    "    scores = cross_val_score(clf, X_flattened, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "        \n",
    "    clf.fit(X_flattened, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X_flattened, y=y, clf=clf)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacking ensemble is illustrated in the figure above. It consists of k-NN, Random Forest and Naive Bayes base classifiers whose predictions are combined by Lostic Regression as a meta-classifier. We can see the blending of decision boundaries achieved by the stacking classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot classifier accuracy    \n",
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(range(4), clf_cv_mean, yerr=clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.xticks(range(7), ['Regression', 'Regression', 'Regression', 'SVC', 'SVC', 'SVC', 'Stacking'])        \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Classifier'); plt.title('Stacking Ensemble');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, sclf, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sclf.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print('RAW CONFUSION MATRIX', '\\n')\n",
    "print(conf_mat, '\\n')\n",
    "print('--------------------\\n')\n",
    "print('Heatmap of Consfusion Matrix:')\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['pos','neg'], yticklabels=['pos','neg'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names=['0','4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "#X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "X = tv_matrix\n",
    "y = df_sm['sentiment']\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# We use our model as defined above\n",
    "sclf.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "# use _predict_proba_lr(testX) with SVC\n",
    "lr_probs = sclf.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('LinearSVC: ROC AUC=%.3f' % (lr_auc))\n",
    "print('\\nROC Curve for Bagging Ensemble with Linear SVC Classifiers:')\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=4)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs, pos_label=4)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Bagging Ensemble with Linear SVC Classifiers')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.title('ROC Curve for Bagging Ensemble with Linear SVC Classifiers')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of code for Milestone 2. Please see top of document for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results\n",
    "\n",
    "## Evaluation Metrics of different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Encoding, 3000 rows, 1200 features selected with Extra Trees.\n",
    "\n",
    "<table><tr><th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Classifier Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</th><th>&nbsp;&nbsp;&nbsp;Accuracy&nbsp;&nbsp;&nbsp;</th><th>&nbsp;&nbsp;&nbsp;F1-Score&nbsp;&nbsp;&nbsp;</th><th>&nbsp;&nbsp;&nbsp;ROC/AUC&nbsp;&nbsp;&nbsp;</th><th>&nbsp;&nbsp;&nbsp;Test Set Errors&nbsp;&nbsp;&nbsp;</th></tr>\n",
    "    <tr><td>Logistic Regression</td><td>0.69</td><td>0.70</td><td>0.737</td><td>0.17</td></tr>\n",
    "    <tr><td>LinearSVC</td><td>0.69</td><td>0.70</td><td>0.733</td><td>0.18</td></tr>\n",
    "    <tr><td>MultinomialNB</td><td>0.69</td><td>0.69</td><td>0.736</td><td>0.33</td></tr>\n",
    "    <tr><td>Random Forest</td><td>0.64</td><td>0.66</td><td>0.724</td><td>0.35</td></tr>\n",
    "    <tr><td>Bagging/Regression</td><td>0.68</td><td>0.68</td><td>0.730</td><td>0.31</td></tr>\n",
    "    <tr><td>Bagging/LinearSVC</td><td>0.69</td><td>0.79</td><td>0.727</td><td>0.32</td></tr>\n",
    "    <tr><td>Boosting/Regression</td><td>0.68</td><td>0.67</td><td>0.722</td><td>0.34</td></tr>\n",
    "    <tr><td>Stacking</td><td>0.59</td><td>0.69</td><td>0.652</td><td>0.30</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blank table for when I run it again\n",
    "\n",
    "<table><tr><th>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Classifier Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</th><th>&nbsp;&nbsp;&nbsp;Accuracy&nbsp;&nbsp;&nbsp;</th><th>&nbsp;&nbsp;&nbsp;F1-Score&nbsp;&nbsp;&nbsp;</th><th>&nbsp;&nbsp;&nbsp;ROC/AUC&nbsp;&nbsp;&nbsp;</th><th>&nbsp;&nbsp;&nbsp;Test Set Errors&nbsp;&nbsp;&nbsp;</th></tr>\n",
    "    <tr><td>Logistic Regression</td><td></td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>LinearSVC</td><td></td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>MultinomialNB</td><td></td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>Random Forest</td><td></td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>Bagging/Regression</td><td></td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>Bagging/LinearSVC</td><td></td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>Boosting/Regression</td><td></td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>Stacking</td><td></td><td></td><td></td><td></td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "-----------------------\n",
    "\n",
    "-----------------------\n",
    "\n",
    "----------------------\n",
    "\n",
    "----------------------\n",
    "\n",
    "---------------------\n",
    "#\n",
    "#\n",
    "#\n",
    "### End of working working file.\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Afinn\n",
    "\n",
    "As a quick and dirty sanity check, I've set up Afinn in the early stages of data cleaning, and intend to keep a little record of Afinn's performance, as I increase the rigour of the data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.array(df_sm['text_nav'])\n",
    "sentiments = np.array(df_sm['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "#train_texts = texts[:10000]\n",
    "#train_sentiments = sentiments[:10000]\n",
    "\n",
    "#test_texts = texts[40000:60000]\n",
    "#test_sentiments = sentiments[40000:60000]\n",
    "sample_ids = [626, 533, 310, 123, 654, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for text_clean, sentiment in zip(texts[sample_ids], sentiments[sample_ids]):\n",
    "#    print('TEXT:', texts)\n",
    "#    print('Actual Sentiment:', sentiment)\n",
    "#    print('Predicted Sentiment polarity:', afn.score(texts))\n",
    "#    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment with Afinn\n",
    "\n",
    "#sentiment_polarity = [afn.score(Text) for Text in normalized_texts]\n",
    "#predicted_sentiments = ['positive' if score >= 1.0 else 'negative' for score in sentiment_polarity]\n",
    "#predicted_sentiments = [4 if score >= 1.0 else 0 for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meu.display_model_performance_metrics(true_labels=test_texts, predicted_labels=predicted_sentiments, \n",
    "#                                  classes=['positive', 'negative'])\n",
    "#meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "#                                  classes=[4, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking cleaning with Afinn\n",
    "\n",
    "I'm curious about how deeper cleaning affects predicitive models. So I set up Afinn after the very first round of data cleaning, and am going to track results here in the markdown. For simplicity, I will monitor the effects of different levels of cleaning on \"weighted avg f1-score\"\n",
    "\n",
    "Round 1, most basic cleaning, 20000 rows:  0.63\n",
    "\n",
    "Round 2, include normalization, 20000 rows: 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "I9_JnCudnSCo",
    "outputId": "e6b101dc-9e29-49c5-e721-777e4557609a"
   },
   "outputs": [],
   "source": [
    "df_sm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aa6qxMhqnSCo"
   },
   "source": [
    "## Save to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFdiqd0WnSCy"
   },
   "outputs": [],
   "source": [
    "#df.to_sql('df_sm', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWaPk57LqDRV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tImPFlPIqG5f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "csml1010-pete-gray.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loi_f63soYVr"
   },
   "source": [
    "# CSML1010 Project Working Copy\n",
    "# Sentiment Analysis with the Sentiment140 dataset\n",
    "## Pete Gray\n",
    "\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4NUyQZQnSBp"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i72mPmFnSBq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import model_evaluation_utils as meu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdxmVNijmdOs"
   },
   "source": [
    "# Adjust pandas display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTTBG34mmWF0"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.precision = 2\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DB0TDJgcm8bj"
   },
   "source": [
    "# Import matplotlib and seaborn and adjust defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFdcwFgZm6Zl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjQmJLg6nSBw"
   },
   "source": [
    "## Read data from local filesystem and csv source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T18:25:50.316246Z",
     "start_time": "2018-12-11T18:25:37.226960Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4TSsbsKqnSBy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MM12Q5qNnSB2"
   },
   "source": [
    "Check data with quick visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "ALDoZOPAnSB4",
    "outputId": "2b0b817e-5dce-40ac-df23-e2be2169a50d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599994</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599995</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599996</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599997</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599998</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "0        scotthamilton     \n",
       "1        mattycus          \n",
       "2        ElleCTF           \n",
       "3        Karoli            \n",
       "4        joy_wolf          \n",
       "...           ...          \n",
       "1599994  AmandaMarie1028   \n",
       "1599995  TheWDBoards       \n",
       "1599996  bpbabe            \n",
       "1599997  tinydiamondz      \n",
       "1599998  RyanTrevMorris    \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!     \n",
       "1        @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                           \n",
       "2        my whole body feels itchy and like its on fire                                                                      \n",
       "3        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.      \n",
       "4        @Kwesidei not the whole crew                                                                                        \n",
       "...                                ...                                                                                       \n",
       "1599994  Just woke up. Having no school is the best feeling ever                                                             \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta                                      \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me for details                                                            \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                                                    \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                                                       \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJhQcLA0u6CJ"
   },
   "source": [
    "## Give dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kS9biLKkt49y"
   },
   "outputs": [],
   "source": [
    "df.columns = ['sentiment', 'ID', 'Time', 'none', 'username', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2CVq6zMunwkN",
    "outputId": "9902caab-b5aa-429b-b572-b6e898af9ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    1599999\n",
       "ID           1599999\n",
       "Time         1599999\n",
       "none         1599999\n",
       "username     1599999\n",
       "Text         1599999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VSn5zWau-3L"
   },
   "source": [
    "## Now it has columns, this seems better.\n",
    "#\n",
    "## We have to cut this down to size, for iterative development.\n",
    "##\n",
    "## Don't forget to get rid of this!!! When crunching whole huge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    100000\n",
       "ID           100000\n",
       "Time         100000\n",
       "none         100000\n",
       "username     100000\n",
       "Text         100000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[750000:850000]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5gpJp0ULn_mZ",
    "outputId": "10669616-4836-443d-f214-a8e30b637e92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment', 'ID', 'Time', 'none', 'username', 'Text']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [col for col in df.columns if not col.startswith('self')]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Back to office to empty aircon water tank  empty office gives too much time for reflection',\n",
       "       '@ToxicMelvin Too late  However it works now. Am really happy!',\n",
       "       \"@exljbris it can't connect \",\n",
       "       'Missing my 20yr old baby-moved to WA. ',\n",
       "       \"@SaulaSmurf How old's ur bro?? mine was 15 when it happened... he looked horrible after the accident \",\n",
       "       'I miss caitlin already ',\n",
       "       \"I'm bored at the dr's office for my mommy. And I miss my jeremy. He works far away now. \",\n",
       "       'nothing on fucking tv to watch. i hate not having my fucking iPod or iMac or my god damn fucking phone. im falling the fuck apart. ',\n",
       "       \"I'm gonna have to give away my dog.  &lt;3\",\n",
       "       'I forgot to eat cookies at the barn today  Travesty!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = np.array(df['Text'])\n",
    "sentiments = np.array(df['sentiment'])\n",
    "raw_text[5:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ig6ds2OinSB8"
   },
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seHULqzsnSB-"
   },
   "outputs": [],
   "source": [
    "#import sqlite3\n",
    "#con = sqlite3.connect('documents.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxsdwx6AnSCC"
   },
   "source": [
    "## Save subreddit category info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "nROV-K1qnSCE",
    "outputId": "06fbdb29-9bd4-4e2b-8ab5-d77fc2ce3fa6"
   },
   "outputs": [],
   "source": [
    "#pd.read_csv(\"testdata.manual.2009.06.14.csv\").to_sql(\"sentiment-labels\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7udZhKkJnSCI"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-B4ADb4QCuL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APqmmwo8nSCJ"
   },
   "source": [
    "## Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctg2OlsXnSCK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(s):\n",
    "    s = s.replace(r'<lb>', \"\\n\")\n",
    "    s = s.replace(r'<tab>', \"\\i\")\n",
    "    s = re.sub(r'<br */*>', \"\\n\", s)\n",
    "    s = s.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    # markdown urls\n",
    "    s = re.sub(r'\\(https*://[^\\)]*\\)', \"\", s)\n",
    "    # normal urls\n",
    "    s = re.sub(r'https*://[^\\s]*', \"\", s)\n",
    "    s = re.sub(r'_+', ' ', s)\n",
    "    s = re.sub(r'\"+', '\"', s)\n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmUAz8ConSCP"
   },
   "source": [
    "## Create new column in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0M6eW8LnSCQ"
   },
   "outputs": [],
   "source": [
    "df[\"selftext_clean\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIWWygCBnSCU"
   },
   "source": [
    "# Iterate and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "177mRPBcnSCU",
    "outputId": "76c4023e-6491-41ba-ad12-9d1b34a313ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 750000\n",
      "processed: 760000\n",
      "processed: 770000\n",
      "processed: 780000\n",
      "processed: 790000\n",
      "processed: 800000\n",
      "processed: 810000\n",
      "processed: 820000\n",
      "processed: 830000\n",
      "processed: 840000\n"
     ]
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if i % 10000 == 0:\n",
    "            print('processed:'.format(i), i)\n",
    "    df.at[i, \"selftext_clean\"] = clean(row.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ecr4awlnSCU"
   },
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "ukxXWf7onSCU",
    "outputId": "e2b12d10-743f-47e3-9362-51167b40b130"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>none</th>\n",
       "      <th>username</th>\n",
       "      <th>Text</th>\n",
       "      <th>selftext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>750000</td>\n",
       "      <td>0</td>\n",
       "      <td>2285370823</td>\n",
       "      <td>Mon Jun 22 15:02:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xbeautifulmessx</td>\n",
       "      <td>@Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.</td>\n",
       "      <td>@Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750001</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371185</td>\n",
       "      <td>Mon Jun 22 15:02:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thefirstsight</td>\n",
       "      <td>@rose_7 Ohh poor jan  please tell her that if she cans, send us an email!!</td>\n",
       "      <td>@rose 7 Ohh poor jan  please tell her that if she cans, send us an email!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750002</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371495</td>\n",
       "      <td>Mon Jun 22 15:02:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Sarah2713</td>\n",
       "      <td>Finally home from work...It was a looong day!! And it's only Monday</td>\n",
       "      <td>Finally home from work...It was a looong day!! And it's only Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750003</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371762</td>\n",
       "      <td>Mon Jun 22 15:02:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dierockerfrau</td>\n",
       "      <td>im very sad 4 chantelle and tom</td>\n",
       "      <td>im very sad 4 chantelle and tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750004</td>\n",
       "      <td>0</td>\n",
       "      <td>2285372377</td>\n",
       "      <td>Mon Jun 22 15:02:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>alexbates</td>\n",
       "      <td>I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste</td>\n",
       "      <td>I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Time      none  \\\n",
       "750000  0          2285370823  Mon Jun 22 15:02:49 PDT 2009  NO_QUERY   \n",
       "750001  0          2285371185  Mon Jun 22 15:02:51 PDT 2009  NO_QUERY   \n",
       "750002  0          2285371495  Mon Jun 22 15:02:52 PDT 2009  NO_QUERY   \n",
       "750003  0          2285371762  Mon Jun 22 15:02:54 PDT 2009  NO_QUERY   \n",
       "750004  0          2285372377  Mon Jun 22 15:02:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "               username  \\\n",
       "750000  xbeautifulmessx   \n",
       "750001  thefirstsight     \n",
       "750002  Sarah2713         \n",
       "750003  dierockerfrau     \n",
       "750004  alexbates         \n",
       "\n",
       "                                                                                                                                     Text  \\\n",
       "750000  @Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.    \n",
       "750001  @rose_7 Ohh poor jan  please tell her that if she cans, send us an email!!                                                          \n",
       "750002  Finally home from work...It was a looong day!! And it's only Monday                                                                 \n",
       "750003  im very sad 4 chantelle and tom                                                                                                     \n",
       "750004  I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste      \n",
       "\n",
       "                                                                                                                           selftext_clean  \n",
       "750000  @Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.   \n",
       "750001  @rose 7 Ohh poor jan  please tell her that if she cans, send us an email!!                                                         \n",
       "750002  Finally home from work...It was a looong day!! And it's only Monday                                                                \n",
       "750003  im very sad 4 chantelle and tom                                                                                                    \n",
       "750004  I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste     "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis with Afinn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.array(df['Text'])\n",
    "sentiments = np.array(df['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "train_texts = texts[:10000]\n",
    "train_sentiments = sentiments[:10000]\n",
    "\n",
    "test_texts = texts[45000:55000]\n",
    "test_sentiments = sentiments[45000:55000]\n",
    "sample_review_ids = [5626, 3533, 6010, 123, 2654, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: @Lee_Knight lmao! thanks Lee XD, would u like to join in our craziness as well lolol  ROFL come &amp; join we don't bite.. not hard anyway! ;)\n",
      "Actual Sentiment: 4\n",
      "Predicted Sentiment polarity: 19.0\n",
      "------------------------------------------------------------\n",
      "TEXT: @ItsJayRabBaby Oh no!!  RIP\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: -1.0\n",
      "------------------------------------------------------------\n",
      "TEXT: I got sunburn today haha. i was outside the wholeee day SKATINGGG \n",
      "Actual Sentiment: 4\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n",
      "TEXT: he stuck on my mind arrgh! please dont treat me like that \n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 1.0\n",
      "------------------------------------------------------------\n",
      "TEXT: I want my Mini 10v now....waiting takes too long. \n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 1.0\n",
      "------------------------------------------------------------\n",
      "TEXT: Charlie lost an angel today   Very sad   http://twurl.nl/k5bw5m\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: -5.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for Text, sentiment in zip(test_texts[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('TEXT:', Text)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', afn.score(Text))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment with Afinn\n",
    "\n",
    "\n",
    "sentiment_polarity = [afn.score(Text) for Text in test_texts]\n",
    "#predicted_sentiments = ['positive' if score >= 1.0 else 'negative' for score in sentiment_polarity]\n",
    "predicted_sentiments = [4 if score >= 1.0 else 0 for score in sentiment_polarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-8ba345224254>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#                                  classes=['positive', 'negative'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m meu.display_model_performance_metrics(true_labels=test_texts, predicted_labels=predicted_sentiments, \n\u001b[1;32m----> 4\u001b[1;33m                                   classes=[4, 0])\n\u001b[0m",
      "\u001b[1;32m~\\CSML1010\\nlp-lifecycle-project-fall2019\\model_evaluation_utils.py\u001b[0m in \u001b[0;36mdisplay_model_performance_metrics\u001b[1;34m(true_labels, predicted_labels, classes)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Performance metrics:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nModel Classification report:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\CSML1010\\nlp-lifecycle-project-fall2019\\model_evaluation_utils.py\u001b[0m in \u001b[0;36mget_metrics\u001b[1;34m(true_labels, predicted_labels)\u001b[0m\n\u001b[0;32m     27\u001b[0m                         metrics.precision_score(true_labels, \n\u001b[0;32m     28\u001b[0m                                                \u001b[0mpredicted_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                                                average='weighted'),\n\u001b[0m\u001b[0;32m     30\u001b[0m                         4))\n\u001b[0;32m     31\u001b[0m     print('Recall:', np.round(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1567\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'precision'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1570\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1415\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# Check that we don't mix string type with number type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mix of label input types (string and number)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "#meu.display_model_performance_metrics(true_labels=test_texts, predicted_labels=predicted_sentiments, \n",
    "#                                  classes=['positive', 'negative'])\n",
    "meu.display_model_performance_metrics(true_labels=test_texts, predicted_labels=predicted_sentiments, \n",
    "                                  classes=[4, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L75iqChDnSCe"
   },
   "source": [
    "#### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynhqSwDunSCe"
   },
   "source": [
    "## Load spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMNfxFA1nSCe"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1l-paPPnSCe"
   },
   "source": [
    "## Iterate over all rows and perform NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "7IGiQ5hfnSCe",
    "outputId": "5170760f-a771-4c10-8a66-51fc279ba9d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750000\n",
      "751000\n",
      "752000\n",
      "753000\n",
      "754000\n",
      "755000\n",
      "756000\n",
      "757000\n",
      "758000\n",
      "759000\n",
      "760000\n",
      "761000\n",
      "762000\n",
      "763000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1c734ccf6eaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"selftext_clean\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"selftext_clean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"selftext_clean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0madjectives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mnouns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mXhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36m_get_moments\u001b[1;34m(ops, X)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_get_moments_reproduce_bug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-08\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"f\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if(row[\"selftext_clean\"] and len(str(row[\"selftext_clean\"])) < 1000000):\n",
    "        doc = nlp(str(row[\"selftext_clean\"]))\n",
    "        adjectives = []\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        lemmas = []\n",
    "\n",
    "        for token in doc:\n",
    "            lemmas.append(token.lemma_)\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "                nouns.append(token.lemma_)\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_)\n",
    "                \n",
    "        df.at[i, \"selftext_lemma\"] = \" \".join(lemmas)                \n",
    "        df.at[i, \"selftext_nouns\"] = \" \".join(nouns)\n",
    "        df.at[i, \"selftext_adjectives\"] = \" \".join(adjectives)\n",
    "        df.at[i, \"selftext_verbs\"] = \" \".join(verbs)\n",
    "        df.at[i, \"selftext_nav\"] = \" \".join(nouns+adjectives+verbs)\n",
    "        df.at[i, \"no_tokens\"] = len(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQdL3PkynSCo"
   },
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "I9_JnCudnSCo",
    "outputId": "e6b101dc-9e29-49c5-e721-777e4557609a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>none</th>\n",
       "      <th>username</th>\n",
       "      <th>Text</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>selftext_lemma</th>\n",
       "      <th>selftext_nouns</th>\n",
       "      <th>selftext_adjectives</th>\n",
       "      <th>selftext_verbs</th>\n",
       "      <th>selftext_nav</th>\n",
       "      <th>no_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>750000</td>\n",
       "      <td>0</td>\n",
       "      <td>2285370823</td>\n",
       "      <td>Mon Jun 22 15:02:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xbeautifulmessx</td>\n",
       "      <td>@Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.</td>\n",
       "      <td>@Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.</td>\n",
       "      <td>@Idristwilight -PRON- can post HAN when -PRON- want . -PRON- be great ! -PRON- be still work on TLD though . -PRON- get a little distracted so sorry .</td>\n",
       "      <td>@Idristwilight HAN TLD</td>\n",
       "      <td>great distracted sorry</td>\n",
       "      <td>can post want work get</td>\n",
       "      <td>@Idristwilight HAN TLD great distracted sorry can post want work get</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750001</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371185</td>\n",
       "      <td>Mon Jun 22 15:02:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thefirstsight</td>\n",
       "      <td>@rose_7 Ohh poor jan  please tell her that if she cans, send us an email!!</td>\n",
       "      <td>@rose 7 Ohh poor jan  please tell her that if she cans, send us an email!!</td>\n",
       "      <td>@rose 7 Ohh poor jan   please tell -PRON- that if -PRON- can , send -PRON- an email ! !</td>\n",
       "      <td>Ohh jan email</td>\n",
       "      <td>poor</td>\n",
       "      <td>tell can send</td>\n",
       "      <td>Ohh jan email poor tell can send</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750002</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371495</td>\n",
       "      <td>Mon Jun 22 15:02:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Sarah2713</td>\n",
       "      <td>Finally home from work...It was a looong day!! And it's only Monday</td>\n",
       "      <td>Finally home from work...It was a looong day!! And it's only Monday</td>\n",
       "      <td>finally home from work ... -PRON- be a looong day ! ! and -PRON- be only Monday</td>\n",
       "      <td>work day Monday</td>\n",
       "      <td>looong</td>\n",
       "      <td></td>\n",
       "      <td>work day Monday looong</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750003</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371762</td>\n",
       "      <td>Mon Jun 22 15:02:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dierockerfrau</td>\n",
       "      <td>im very sad 4 chantelle and tom</td>\n",
       "      <td>im very sad 4 chantelle and tom</td>\n",
       "      <td>-PRON- be very sad 4 chantelle and tom</td>\n",
       "      <td>chantelle tom</td>\n",
       "      <td>sad</td>\n",
       "      <td>be</td>\n",
       "      <td>chantelle tom sad be</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750004</td>\n",
       "      <td>0</td>\n",
       "      <td>2285372377</td>\n",
       "      <td>Mon Jun 22 15:02:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>alexbates</td>\n",
       "      <td>I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste</td>\n",
       "      <td>I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste</td>\n",
       "      <td>-PRON- chat with someone on the online Apple store and -PRON- say -PRON- would be well to buy a new one . -PRON- do not have $ 200 to waste</td>\n",
       "      <td>Apple store</td>\n",
       "      <td>online well new</td>\n",
       "      <td>chat say would buy waste</td>\n",
       "      <td>Apple store online well new chat say would buy waste</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Time      none  \\\n",
       "750000  0          2285370823  Mon Jun 22 15:02:49 PDT 2009  NO_QUERY   \n",
       "750001  0          2285371185  Mon Jun 22 15:02:51 PDT 2009  NO_QUERY   \n",
       "750002  0          2285371495  Mon Jun 22 15:02:52 PDT 2009  NO_QUERY   \n",
       "750003  0          2285371762  Mon Jun 22 15:02:54 PDT 2009  NO_QUERY   \n",
       "750004  0          2285372377  Mon Jun 22 15:02:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "               username  \\\n",
       "750000  xbeautifulmessx   \n",
       "750001  thefirstsight     \n",
       "750002  Sarah2713         \n",
       "750003  dierockerfrau     \n",
       "750004  alexbates         \n",
       "\n",
       "                                                                                                                                     Text  \\\n",
       "750000  @Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.    \n",
       "750001  @rose_7 Ohh poor jan  please tell her that if she cans, send us an email!!                                                          \n",
       "750002  Finally home from work...It was a looong day!! And it's only Monday                                                                 \n",
       "750003  im very sad 4 chantelle and tom                                                                                                     \n",
       "750004  I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste      \n",
       "\n",
       "                                                                                                                           selftext_clean  \\\n",
       "750000  @Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.    \n",
       "750001  @rose 7 Ohh poor jan  please tell her that if she cans, send us an email!!                                                          \n",
       "750002  Finally home from work...It was a looong day!! And it's only Monday                                                                 \n",
       "750003  im very sad 4 chantelle and tom                                                                                                     \n",
       "750004  I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste      \n",
       "\n",
       "                                                                                                                                                selftext_lemma  \\\n",
       "750000  @Idristwilight -PRON- can post HAN when -PRON- want . -PRON- be great ! -PRON- be still work on TLD though . -PRON- get a little distracted so sorry .   \n",
       "750001  @rose 7 Ohh poor jan   please tell -PRON- that if -PRON- can , send -PRON- an email ! !                                                                  \n",
       "750002  finally home from work ... -PRON- be a looong day ! ! and -PRON- be only Monday                                                                          \n",
       "750003  -PRON- be very sad 4 chantelle and tom                                                                                                                   \n",
       "750004  -PRON- chat with someone on the online Apple store and -PRON- say -PRON- would be well to buy a new one . -PRON- do not have $ 200 to waste              \n",
       "\n",
       "                selftext_nouns     selftext_adjectives  \\\n",
       "750000  @Idristwilight HAN TLD  great distracted sorry   \n",
       "750001  Ohh jan email           poor                     \n",
       "750002  work day Monday         looong                   \n",
       "750003  chantelle tom           sad                      \n",
       "750004  Apple store             online well new          \n",
       "\n",
       "                  selftext_verbs  \\\n",
       "750000  can post want work get     \n",
       "750001  tell can send              \n",
       "750002                             \n",
       "750003  be                         \n",
       "750004  chat say would buy waste   \n",
       "\n",
       "                                                                selftext_nav  \\\n",
       "750000  @Idristwilight HAN TLD great distracted sorry can post want work get   \n",
       "750001  Ohh jan email poor tell can send                                       \n",
       "750002  work day Monday looong                                                 \n",
       "750003  chantelle tom sad be                                                   \n",
       "750004  Apple store online well new chat say would buy waste                   \n",
       "\n",
       "        no_tokens  \n",
       "750000 29.00       \n",
       "750001 20.00       \n",
       "750002 17.00       \n",
       "750003 8.00        \n",
       "750004 31.00       "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aa6qxMhqnSCo"
   },
   "source": [
    "## Save to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFdiqd0WnSCy"
   },
   "outputs": [],
   "source": [
    "df.to_sql('posts_nlp', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWaPk57LqDRV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tImPFlPIqG5f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "csml1010-pete-gray.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSML1010 Day 2 coding exercise\n",
    "## Second attempt, Dec 4 2019\n",
    "## Pete Gray\n",
    "\n",
    "I wasn't able to even crack this one on my old computers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion and Sentiment Analysis\n",
    "Sentiment analysis is perhaps one of the most popular applications of NLP, with a vast number of tutorials, courses, and applications that focus on analyzing sentiments of diverse datasets ranging from corporate surveys to movie reviews. The key aspect of sentiment analysis is to analyze a body of text for understanding the opinion expressed by it. Typically, we quantify this sentiment with a positive or negative value, called polarity. The overall sentiment is often inferred as positive, neutral or negative from the sign of the polarity score.\n",
    "\n",
    "Usually, sentiment analysis works best on text that has a subjective context than on text with only an objective context. Objective text usually depicts some normal statements or facts without expressing any emotion, feelings, or mood. Subjective text contains text that is usually expressed by a human having typical moods, emotions, and feelings. Sentiment analysis is widely used, especially as a part of social media analysis for any domain, be it a business, a recent movie, or a product launch, to understand its reception by the people and what they think of it based on their opinions or, you guessed it, sentiment!\n",
    "\n",
    "\n",
    "Typically, sentiment analysis for text data can be computed on several levels, including on an individual sentence level, paragraph level, or the entire document as a whole. Often, sentiment is computed on the document as a whole or some aggregations are done after computing the sentiment for individual sentences. There are two major approaches to sentiment analysis.\n",
    " - Supervised machine learning or deep learning approaches\n",
    " - Unsupervised lexicon-based approaches\n",
    "\n",
    "For the first approach we typically need pre-labeled data. Hence, we will be focusing on the second approach. For a comprehensive coverage of sentiment analysis, refer to Chapter 7: Analyzing Movie Reviews Sentiment, Practical Machine Learning with Python, Springer\\Apress, 2018. In this scenario, we do not have the convenience of a well-labeled training dataset. Hence, we will need to use unsupervised techniques for predicting the sentiment by using knowledgebases, ontologies, databases, and lexicons that have detailed information, specially curated and prepared just for sentiment analysis. A lexicon is a dictionary, vocabulary, or a book of words. In our case, lexicons are special dictionaries or vocabularies that have been created for analyzing sentiments. Most of these lexicons have a list of positive and negative polar words with some score associated with them, and using various techniques like the position of words, surrounding words, context, parts of speech, phrases, and so on, scores are assigned to the text documents for which we want to compute the sentiment. After aggregating these scores, we get the final sentiment.\n",
    "\n",
    "\n",
    "Various popular lexicons are used for sentiment analysis, including the following.\n",
    "\n",
    "AFINN lexicon\n",
    "Bing Liu’s lexicon\n",
    "MPQA subjectivity lexicon\n",
    "SentiWordNet\n",
    "VADER lexicon\n",
    "TextBlob lexicon\n",
    "This is not an exhaustive list of lexicons that can be leveraged for sentiment analysis, and there are several other lexicons which can be easily obtained from the Internet. Feel free to check out each of these links and explore them. We will be covering two techniques in this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Pre-Processing\n",
    "\n",
    "### Import necessary depencencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import model_evaluation_utils as meu\n",
    "\n",
    "np.set_printoptions(precision=2, linewidth=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and normalize data\n",
    "1.  Cleaning Text - strip HTML\n",
    "2.  Removing accented characters\n",
    "3.  Expanding Contractions\n",
    "4.  Removing Special Characters\n",
    "5.  Lemmatizing text¶\n",
    "6.  Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'movie_reviews_cleaned.csv')\n",
    "\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "train_reviews = reviews[:10000]\n",
    "train_sentiments = sentiments[:10000]\n",
    "\n",
    "test_reviews = reviews[10000:15000]\n",
    "test_sentiments = sentiments[10000:15000]\n",
    "sample_review_ids = [2626, 3533, 3010]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP FOR THE STUDENTS BECAUSE INSTRUCTOR HAS PRE_NORMALIZED AND SAVED THE FILE\n",
    "# normalize dataset (time consuming using spacey pipeline)\n",
    "\"\"\"\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews)\n",
    "norm_train_reviews = tn.normalize_corpus(train_reviews)\n",
    "#output back to a csv file again\n",
    "import csv\n",
    "with open(r'movie_reviews_cleaned.csv', mode='w') as cleaned_file:\n",
    "    csv_writer = csv.writer(cleaned_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(['review', 'sentiment'])\n",
    "    for  text, sent in zip(norm_test_reviews, test_sentiments):\n",
    "        csv_writer.writerow([text, sent])\n",
    "    for  text, sent in zip(norm_train_reviews, train_sentiments):\n",
    "        csv_writer.writerow([text, sent])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ============================================\n",
    "# Part A. Unsupervised (Lexicon) Sentiment Analysis\n",
    "# ============================================\n",
    "## 1.  Sentiment Analysis with AFINN\n",
    "The AFINN lexicon is perhaps one of the simplest and most popular lexicons that can be used extensively for sentiment analysis. Developed and curated by Finn Arup Nielsen, you can find more details on this lexicon in the paper, “A new ANEW: evaluation of a word list for sentiment analysis in microblogs”, proceedings of the ESWC 2011 Workshop. The current version of the lexicon is AFINN-en-165. txt and it contains over 3,300+ words with a polarity score associated with each word. You can find this lexicon at the author’s official GitHub repository along with previous versions of it, including AFINN-111. The author has also created a nice wrapper library on top of this in Python called afinn, which we will be using for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True) \n",
    "\n",
    "# NOTE:  to use afinn score, call the function afn.score(\"text you want the sentiment for\")\n",
    "# the lexicon will be used to compute summary of sentiment for the given text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sentiment for sample reviews\n",
    "We can get a good idea of general sentiment for different sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: film producer hop cameron diaz name help sell picture unfortunately nothing save already capture screen despite beautifully shoot european location solid production design element film fail mostly due awkward unbelievable romance brewster eccleston unplesasant filmgoing experience\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment polarity: 5.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: totally surprised comment forum many review think tony scott make good movie yes highly stylize flashy top entertaining glad least ebert roeper agree movie may not anyone like top dark humor cool action dialog see previously see scott man fire crimson tide enemy state good movie like one like roller coaster ride great soundtrack selection visual style time movie seem pg13 nice see someone not afraid show nudity gory violence explicit dialog not hurt keira super hot even show nipple one either\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: 24.0\n",
      "------------------------------------------------------------\n",
      "REVIEW: bad horror film ever funniest film ever roll one get see film cheap unbeliaveble see really p watch carrot\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: -3.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', afn.score(review))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_polarity = [afn.score(review) for review in test_reviews]\n",
    "predicted_sentiments = ['positive' if score >= 1.0 else 'negative' for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.713\n",
      "Precision: 0.7333\n",
      "Recall: 0.713\n",
      "F1 Score: 0.7074\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.66      0.86      0.75      2470\n",
      "    negative       0.80      0.57      0.67      2530\n",
      "\n",
      "    accuracy                           0.71      5000\n",
      "   macro avg       0.73      0.71      0.71      5000\n",
      "weighted avg       0.73      0.71      0.71      5000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\CSML1010\\nlp-lifecycle-project-fall2019\\exercises\\day2\\model_evaluation_utils.py:62: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "  labels=level_labels),\n",
      "C:\\Users\\Dell\\CSML1010\\nlp-lifecycle-project-fall2019\\exercises\\day2\\model_evaluation_utils.py:64: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "  labels=level_labels))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       2113      357\n",
      "        negative       1078     1452\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis with SentiWordNet\n",
    "SentiWordNet is a lexical resource for opinion mining. SentiWordNet assigns to each synset of WordNet three sentiment scores: positivity, negativity, objectivity. SentiWordNet is described in details in the papers: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Polarity Score: 0.875\n",
      "Negative Polarity Score: 0.125\n",
      "Objective Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "awesome = list(swn.senti_synsets('awesome', 'a'))[0]\n",
    "print('Positive Polarity Score:', awesome.pos_score())\n",
    "print('Negative Polarity Score:', awesome.neg_score())\n",
    "print('Objective Score:', awesome.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model\n",
    "For each word in  the review, add up the sentiment score of words that are NN, VB, JJ, RB if it's in the lexicon dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_sentiwordnet_lexicon(review,\n",
    "                                           verbose=False):\n",
    "\n",
    "    # tokenize and POS tag text tokens\n",
    "    tagged_text = [(token.text, token.tag_) for token in tn.nlp(review)]\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "    # get wordnet synsets based on POS tags\n",
    "    # get sentiment scores if synsets are found\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n",
    "        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n",
    "        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n",
    "        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n",
    "        # if senti-synset is found        \n",
    "        if ss_set:\n",
    "            # add scores for all found synsets\n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    # aggregate final scores\n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        # to display results in a nice table\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n",
    "                                         norm_neg_score, norm_final_score]],\n",
    "                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                             ['Predicted Sentiment', 'Objectivity',\n",
    "                                                              'Positive', 'Negative', 'Overall']], \n",
    "                                                             labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "        \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: film producer hop cameron diaz name help sell picture unfortunately nothing save already capture screen despite beautifully shoot european location solid production design element film fail mostly due awkward unbelievable romance brewster eccleston unplesasant filmgoing experience\n",
      "Actual Sentiment: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.84     0.09     0.07    0.03\n",
      "------------------------------------------------------------\n",
      "REVIEW: totally surprised comment forum many review think tony scott make good movie yes highly stylize flashy top entertaining glad least ebert roeper agree movie may not anyone like top dark humor cool action dialog see previously see scott man fire crimson tide enemy state good movie like one like roller coaster ride great soundtrack selection visual style time movie seem pg13 nice see someone not afraid show nudity gory violence explicit dialog not hurt keira super hot even show nipple one either\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.84     0.08     0.07    0.01\n",
      "------------------------------------------------------------\n",
      "REVIEW: bad horror film ever funniest film ever roll one get see film cheap unbeliaveble see really p watch carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \n",
      "  Predicted Sentiment Objectivity Positive Negative Overall\n",
      "0            positive        0.86     0.07     0.07     0.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)    \n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.6736\n",
      "Precision: 0.6792\n",
      "Recall: 0.6736\n",
      "F1 Score: 0.6716\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.65      0.75      0.70      2470\n",
      "    negative       0.71      0.59      0.65      2530\n",
      "\n",
      "    accuracy                           0.67      5000\n",
      "   macro avg       0.68      0.67      0.67      5000\n",
      "weighted avg       0.68      0.67      0.67      5000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       1863      607\n",
      "        negative       1025     1505\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_vader_lexicon(review, \n",
    "                                    threshold=0.1,\n",
    "                                    verbose=False):\n",
    "    # pre-process text\n",
    "    review = tn.strip_html_tags(review)\n",
    "    review = tn.remove_accented_chars(review)\n",
    "    review = tn.expand_contractions(review)\n",
    "    \n",
    "    # analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # get aggregate scores and final sentiment\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 'positive' if agg_score >= threshold\\\n",
    "                                   else 'negative'\n",
    "    if verbose:\n",
    "        # display detailed sentiment statistics\n",
    "        positive = str(round(scores['pos'], 2)*100)+'%'\n",
    "        final = round(agg_score, 2)\n",
    "        negative = str(round(scores['neg'], 2)*100)+'%'\n",
    "        neutral = str(round(scores['neu'], 2)*100)+'%'\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n",
    "                                        negative, neutral]],\n",
    "                                        columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Polarity Score',\n",
    "                                                                       'Positive', 'Negative', 'Neutral']], \n",
    "                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "    \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sentiment for sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: film producer hop cameron diaz name help sell picture unfortunately nothing save already capture screen despite beautifully shoot european location solid production design element film fail mostly due awkward unbelievable romance brewster eccleston unplesasant filmgoing experience\n",
      "Actual Sentiment: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            negative          -0.34    23.0%    26.0%   51.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: totally surprised comment forum many review think tony scott make good movie yes highly stylize flashy top entertaining glad least ebert roeper agree movie may not anyone like top dark humor cool action dialog see previously see scott man fire crimson tide enemy state good movie like one like roller coaster ride great soundtrack selection visual style time movie seem pg13 nice see someone not afraid show nudity gory violence explicit dialog not hurt keira super hot even show nipple one either\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                         \n",
      "  Predicted Sentiment Polarity Score Positive Negative Neutral\n",
      "0            positive           0.94    32.0%    17.0%   52.0%\n",
      "------------------------------------------------------------\n",
      "REVIEW: bad horror film ever funniest film ever roll one get see film cheap unbeliaveble see really p watch carrot\n",
      "Actual Sentiment: positive\n",
      "     SENTIMENT STATS:                                      \\\n",
      "  Predicted Sentiment Polarity Score             Positive   \n",
      "0            negative          -0.56  14.000000000000002%   \n",
      "\n",
      "                                            \n",
      "              Negative             Neutral  \n",
      "0  28.000000000000004%  57.99999999999999%  \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)    \n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sentiment for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in test_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7038\n",
      "Precision: 0.7152\n",
      "Recall: 0.7038\n",
      "F1 Score: 0.7005\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.66      0.81      0.73      2470\n",
      "    negative       0.77      0.60      0.67      2530\n",
      "\n",
      "    accuracy                           0.70      5000\n",
      "   macro avg       0.71      0.71      0.70      5000\n",
      "weighted avg       0.72      0.70      0.70      5000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       2008      462\n",
      "        negative       1019     1511\n"
     ]
    }
   ],
   "source": [
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "#\n",
    "# End of part 1 - Unsupervised\n",
    "#\n",
    "----------------------------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loi_f63soYVr"
   },
   "source": [
    "# CSML1010 Project Working Copy\n",
    "# Sentiment Analysis with the Sentiment140 dataset\n",
    "## Pete Gray\n",
    "\n",
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4NUyQZQnSBp"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i72mPmFnSBq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2, linewidth=80)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import model_evaluation_utils as meu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdxmVNijmdOs"
   },
   "source": [
    "# Adjust pandas display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTTBG34mmWF0"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 30\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.precision = 2\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DB0TDJgcm8bj"
   },
   "source": [
    "# Import matplotlib and seaborn and adjust defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFdcwFgZm6Zl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjQmJLg6nSBw"
   },
   "source": [
    "## Read data from local filesystem and csv source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T18:25:50.316246Z",
     "start_time": "2018-12-11T18:25:37.226960Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "4TSsbsKqnSBy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MM12Q5qNnSB2"
   },
   "source": [
    "Check data with quick visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "colab_type": "code",
    "id": "ALDoZOPAnSB4",
    "outputId": "2b0b817e-5dce-40ac-df23-e2be2169a50d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599994</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599995</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599996</td>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me for details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599997</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599998</td>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "0        scotthamilton     \n",
       "1        mattycus          \n",
       "2        ElleCTF           \n",
       "3        Karoli            \n",
       "4        joy_wolf          \n",
       "...           ...          \n",
       "1599994  AmandaMarie1028   \n",
       "1599995  TheWDBoards       \n",
       "1599996  bpbabe            \n",
       "1599997  tinydiamondz      \n",
       "1599998  RyanTrevMorris    \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!     \n",
       "1        @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                           \n",
       "2        my whole body feels itchy and like its on fire                                                                      \n",
       "3        @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.      \n",
       "4        @Kwesidei not the whole crew                                                                                        \n",
       "...                                ...                                                                                       \n",
       "1599994  Just woke up. Having no school is the best feeling ever                                                             \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interviews!  â« http://blip.fm/~8bmta                                      \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me for details                                                            \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur                                                    \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H                                                       \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJhQcLA0u6CJ"
   },
   "source": [
    "## Give dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kS9biLKkt49y"
   },
   "outputs": [],
   "source": [
    "df.columns = ['sentiment', 'ID', 'Time', 'none', 'username', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2CVq6zMunwkN",
    "outputId": "9902caab-b5aa-429b-b572-b6e898af9ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    1599999\n",
       "ID           1599999\n",
       "Time         1599999\n",
       "none         1599999\n",
       "username     1599999\n",
       "Text         1599999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VSn5zWau-3L"
   },
   "source": [
    "## Now it has columns, this seems better.\n",
    "#\n",
    "## We have to cut this down to size, for iterative development.\n",
    "##\n",
    "## Don't forget to get rid of this!!! When crunching whole huge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    10000\n",
       "ID           10000\n",
       "Time         10000\n",
       "none         10000\n",
       "username     10000\n",
       "Text         10000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_size = 10000\n",
    "start_row = int(800000-(dev_data_size/2))-1\n",
    "finish_row = int(800000+(dev_data_size/2))-1\n",
    "df_sm = df[start_row:finish_row]\n",
    "df_sm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5gpJp0ULn_mZ",
    "outputId": "10669616-4836-443d-f214-a8e30b637e92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment', 'ID', 'Time', 'none', 'username', 'Text']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [col for col in df.columns if not col.startswith('self')]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['im sitting alone at TTE myself without my two michigan sisters for the first time in a decade.  ',\n",
       "       \"@Julie90210 It took me three attempts but I got it in the end. I'm sorry for your loss  Did you try the DRU thing and iTunes restore?\",\n",
       "       'awake for summer school. ',\n",
       "       \"@AJBombers Just noticed it is an afternoon game, won't be able to make that game sorry, thanks for the offer but I will have to decline.. \",\n",
       "       \"Sorry for the long listening, but I'm too lazy for twitter  and I had to work hard in the last week\",\n",
       "       'Bleurgh...feeling rough ',\n",
       "       'Hey @pcwoessner have to leave   back to work.   (Summer PD 09 live &gt; http://ustre.am/3mgf)',\n",
       "       '@margxwanders awwwwwwwww! busy much?? miss you gaux! ',\n",
       "       'Just got to work ', \"@claireliz81 ....he caught Sanchez's disease \"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = np.array(df_sm['Text'])\n",
    "sentiments = np.array(df_sm['sentiment'])\n",
    "raw_text[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 4, 4, 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[4995:5005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7udZhKkJnSCI"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-B4ADb4QCuL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APqmmwo8nSCJ"
   },
   "source": [
    "## Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctg2OlsXnSCK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(s):\n",
    "    s = str(s).replace(r'<lb>', \"\\n\")\n",
    "    s = s.replace(r'<tab>', \"\\i\")\n",
    "    \n",
    "    # As a sanity check - s = s.replace(r'W', \"Q\")\n",
    "    \n",
    "    s = re.sub(r'<br */*>', \"\\n\", s)\n",
    "    s = s.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    # markdown urls\n",
    "    s = re.sub(r'\\(https*://[^\\)]*\\)', \"\", s)\n",
    "    # normal urls\n",
    "    s = re.sub(r'https*://[^\\s]*', \"\", s)\n",
    "    s = re.sub(r'_+', ' ', s)\n",
    "    s = re.sub(r'\"+', '\"', s)\n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmUAz8ConSCP"
   },
   "source": [
    "## Create new column in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0M6eW8LnSCQ"
   },
   "outputs": [],
   "source": [
    "df_sm[\"text_clean\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIWWygCBnSCU"
   },
   "source": [
    "# Iterate and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "177mRPBcnSCU",
    "outputId": "76c4023e-6491-41ba-ad12-9d1b34a313ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 800000\n"
     ]
    }
   ],
   "source": [
    "for i, row in df_sm.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "            print('processed:'.format(i), i)\n",
    "    df_sm.at[i, \"text_clean\"] = clean(row.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ecr4awlnSCU"
   },
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "ukxXWf7onSCU",
    "outputId": "e2b12d10-743f-47e3-9362-51167b40b130"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>none</th>\n",
       "      <th>username</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>794999</td>\n",
       "      <td>0</td>\n",
       "      <td>2327192646</td>\n",
       "      <td>Thu Jun 25 08:02:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>quiz_master</td>\n",
       "      <td>Was having dinner with parents downstairs in Dining Room, they started watching 'Baba Ramdev Yoga' thingy, so I'm back to my room  .</td>\n",
       "      <td>Was having dinner with parents downstairs in Dining Room, they started watching 'Baba Ramdev Yoga' thingy, so I'm back to my room  .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795000</td>\n",
       "      <td>0</td>\n",
       "      <td>2327193206</td>\n",
       "      <td>Thu Jun 25 08:02:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>djcampos</td>\n",
       "      <td>Blah 5am still up  daang I got deep problems</td>\n",
       "      <td>Blah 5am still up  daang I got deep problems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795001</td>\n",
       "      <td>0</td>\n",
       "      <td>2327193455</td>\n",
       "      <td>Thu Jun 25 08:02:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RKF</td>\n",
       "      <td>@jenspeedy I would suggest avoiding 360 Living.  Not goodness    Try contacting Scott at MKCC mkccrenovations@rogers.com 905-303-9009</td>\n",
       "      <td>@jenspeedy I would suggest avoiding 360 Living.  Not goodness    Try contacting Scott at MKCC mkccrenovations@rogers.com 905-303-9009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795002</td>\n",
       "      <td>0</td>\n",
       "      <td>2327193641</td>\n",
       "      <td>Thu Jun 25 08:02:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AnaHertz</td>\n",
       "      <td>@alexbroun I didn't convince myself I was fat and ugly someone else did a pretty good job of that. Its a long story sorry</td>\n",
       "      <td>@alexbroun I didn't convince myself I was fat and ugly someone else did a pretty good job of that. Its a long story sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795003</td>\n",
       "      <td>0</td>\n",
       "      <td>2327193806</td>\n",
       "      <td>Thu Jun 25 08:02:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yenafer</td>\n",
       "      <td>@spotzle @jstarrh check on sunscreen, snacks, towels, suits, kids  drinks, bags and tape for boys cast, camera.  Need toys, chairs ugh :-/</td>\n",
       "      <td>@spotzle @jstarrh check on sunscreen, snacks, towels, suits, kids  drinks, bags and tape for boys cast, camera.  Need toys, chairs ugh :-/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Time      none  \\\n",
       "794999  0          2327192646  Thu Jun 25 08:02:13 PDT 2009  NO_QUERY   \n",
       "795000  0          2327193206  Thu Jun 25 08:02:16 PDT 2009  NO_QUERY   \n",
       "795001  0          2327193455  Thu Jun 25 08:02:17 PDT 2009  NO_QUERY   \n",
       "795002  0          2327193641  Thu Jun 25 08:02:18 PDT 2009  NO_QUERY   \n",
       "795003  0          2327193806  Thu Jun 25 08:02:18 PDT 2009  NO_QUERY   \n",
       "\n",
       "           username  \\\n",
       "794999  quiz_master   \n",
       "795000  djcampos      \n",
       "795001  RKF           \n",
       "795002  AnaHertz      \n",
       "795003  yenafer       \n",
       "\n",
       "                                                                                                                                              Text  \\\n",
       "794999  Was having dinner with parents downstairs in Dining Room, they started watching 'Baba Ramdev Yoga' thingy, so I'm back to my room  .         \n",
       "795000  Blah 5am still up  daang I got deep problems                                                                                                 \n",
       "795001  @jenspeedy I would suggest avoiding 360 Living.  Not goodness    Try contacting Scott at MKCC mkccrenovations@rogers.com 905-303-9009        \n",
       "795002  @alexbroun I didn't convince myself I was fat and ugly someone else did a pretty good job of that. Its a long story sorry                    \n",
       "795003  @spotzle @jstarrh check on sunscreen, snacks, towels, suits, kids  drinks, bags and tape for boys cast, camera.  Need toys, chairs ugh :-/   \n",
       "\n",
       "                                                                                                                                        text_clean  \n",
       "794999  Was having dinner with parents downstairs in Dining Room, they started watching 'Baba Ramdev Yoga' thingy, so I'm back to my room  .        \n",
       "795000  Blah 5am still up  daang I got deep problems                                                                                                \n",
       "795001  @jenspeedy I would suggest avoiding 360 Living.  Not goodness    Try contacting Scott at MKCC mkccrenovations@rogers.com 905-303-9009       \n",
       "795002  @alexbroun I didn't convince myself I was fat and ugly someone else did a pretty good job of that. Its a long story sorry                   \n",
       "795003  @spotzle @jstarrh check on sunscreen, snacks, towels, suits, kids  drinks, bags and tape for boys cast, camera.  Need toys, chairs ugh :-/  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional pre-processing: tokenization, removing extra whitespaces, lower casing and more advanced operations like spelling corrections, grammatical error corrections, removing repeated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm[\"text_normalized\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 795000\n",
      "processed: 796000\n",
      "processed: 797000\n",
      "processed: 798000\n",
      "processed: 799000\n",
      "processed: 800000\n",
      "processed: 801000\n",
      "processed: 802000\n",
      "processed: 803000\n",
      "processed: 804000\n"
     ]
    }
   ],
   "source": [
    "for i, row in df_sm.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "            print('processed:'.format(i), i)\n",
    "    df_sm.at[i, \"text_normalized\"] = normalize_corpus(row.text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>none</th>\n",
       "      <th>username</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>794999</td>\n",
       "      <td>0</td>\n",
       "      <td>2327192646</td>\n",
       "      <td>Thu Jun 25 08:02:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>quiz_master</td>\n",
       "      <td>Was having dinner with parents downstairs in Dining Room, they started watching 'Baba Ramdev Yoga' thingy, so I'm back to my room  .</td>\n",
       "      <td>Was having dinner with parents downstairs in Dining Room, they started watching 'Baba Ramdev Yoga' thingy, so I'm back to my room  .</td>\n",
       "      <td>dinner parents downstairs dining room started watching baba ramdev yoga ' thingy , ' back room .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795000</td>\n",
       "      <td>0</td>\n",
       "      <td>2327193206</td>\n",
       "      <td>Thu Jun 25 08:02:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>djcampos</td>\n",
       "      <td>Blah 5am still up  daang I got deep problems</td>\n",
       "      <td>Blah 5am still up  daang I got deep problems</td>\n",
       "      <td>blah 5am still daang got deep problems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795001</td>\n",
       "      <td>0</td>\n",
       "      <td>2327193455</td>\n",
       "      <td>Thu Jun 25 08:02:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RKF</td>\n",
       "      <td>@jenspeedy I would suggest avoiding 360 Living.  Not goodness    Try contacting Scott at MKCC mkccrenovations@rogers.com 905-303-9009</td>\n",
       "      <td>@jenspeedy I would suggest avoiding 360 Living.  Not goodness    Try contacting Scott at MKCC mkccrenovations@rogers.com 905-303-9009</td>\n",
       "      <td>jenspeedy would suggest avoiding 360 living goodness try contacting scott mkcc mkccrenovations @ rogers . com 905 - 303 - 9009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795002</td>\n",
       "      <td>0</td>\n",
       "      <td>2327193641</td>\n",
       "      <td>Thu Jun 25 08:02:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AnaHertz</td>\n",
       "      <td>@alexbroun I didn't convince myself I was fat and ugly someone else did a pretty good job of that. Its a long story sorry</td>\n",
       "      <td>@alexbroun I didn't convince myself I was fat and ugly someone else did a pretty good job of that. Its a long story sorry</td>\n",
       "      <td>alexbroun didnt convince fat ugly someone else pretty good job . long story sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795003</td>\n",
       "      <td>0</td>\n",
       "      <td>2327193806</td>\n",
       "      <td>Thu Jun 25 08:02:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yenafer</td>\n",
       "      <td>@spotzle @jstarrh check on sunscreen, snacks, towels, suits, kids  drinks, bags and tape for boys cast, camera.  Need toys, chairs ugh :-/</td>\n",
       "      <td>@spotzle @jstarrh check on sunscreen, snacks, towels, suits, kids  drinks, bags and tape for boys cast, camera.  Need toys, chairs ugh :-/</td>\n",
       "      <td>spotzle jstarrh check sunscreen , snacks , towels , suits , kids drinks , bags tape boys cast , camera . need toys , chairs ugh :-/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804994</td>\n",
       "      <td>4</td>\n",
       "      <td>1468599653</td>\n",
       "      <td>Tue Apr 07 02:39:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>danalynbyers</td>\n",
       "      <td>@lbran, thanks for sending us the package - got it this morning!</td>\n",
       "      <td>@lbran, thanks for sending us the package - got it this morning!</td>\n",
       "      <td>lbran thanks sending us package - got morning !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804995</td>\n",
       "      <td>4</td>\n",
       "      <td>1468599688</td>\n",
       "      <td>Tue Apr 07 02:39:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joscelinyeo</td>\n",
       "      <td>@ickleoriental hahahha.. U obviously don't hv one!! But maybe u can give me advice?  Fdw.. Foreign domestic worker</td>\n",
       "      <td>@ickleoriental hahahha.. U obviously don't hv one!! But maybe u can give me advice?  Fdw.. Foreign domestic worker</td>\n",
       "      <td>ickleoriental hahahha . u obviously ' hv one !! maybe u give advice ? fdw .. foreign domestic worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804996</td>\n",
       "      <td>4</td>\n",
       "      <td>1468599702</td>\n",
       "      <td>Tue Apr 07 02:39:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>serengetisunset</td>\n",
       "      <td>@juliekoh It's an internet term, but it's spilled over into common use, in real life</td>\n",
       "      <td>@juliekoh It's an internet term, but it's spilled over into common use, in real life</td>\n",
       "      <td>juliekoh internet term , ' spilled common use , real life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804997</td>\n",
       "      <td>4</td>\n",
       "      <td>1468599765</td>\n",
       "      <td>Tue Apr 07 02:39:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>broombeck</td>\n",
       "      <td>new day.... NEW TRACK!!!!</td>\n",
       "      <td>new day.... NEW TRACK!!!!</td>\n",
       "      <td>new day .. new track !!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804998</td>\n",
       "      <td>4</td>\n",
       "      <td>1468599793</td>\n",
       "      <td>Tue Apr 07 02:39:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>aptronym</td>\n",
       "      <td>@foodieguide Okay we need to have a competition to see whether you or I have the dodgiest Cantonese. I bet I win</td>\n",
       "      <td>@foodieguide Okay we need to have a competition to see whether you or I have the dodgiest Cantonese. I bet I win</td>\n",
       "      <td>foodieguide okay need competition see whether dodgiest cantonese bet win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Time      none  \\\n",
       "794999  0          2327192646  Thu Jun 25 08:02:13 PDT 2009  NO_QUERY   \n",
       "795000  0          2327193206  Thu Jun 25 08:02:16 PDT 2009  NO_QUERY   \n",
       "795001  0          2327193455  Thu Jun 25 08:02:17 PDT 2009  NO_QUERY   \n",
       "795002  0          2327193641  Thu Jun 25 08:02:18 PDT 2009  NO_QUERY   \n",
       "795003  0          2327193806  Thu Jun 25 08:02:18 PDT 2009  NO_QUERY   \n",
       "...    ..                 ...                           ...       ...   \n",
       "804994  4          1468599653  Tue Apr 07 02:39:03 PDT 2009  NO_QUERY   \n",
       "804995  4          1468599688  Tue Apr 07 02:39:04 PDT 2009  NO_QUERY   \n",
       "804996  4          1468599702  Tue Apr 07 02:39:04 PDT 2009  NO_QUERY   \n",
       "804997  4          1468599765  Tue Apr 07 02:39:06 PDT 2009  NO_QUERY   \n",
       "804998  4          1468599793  Tue Apr 07 02:39:07 PDT 2009  NO_QUERY   \n",
       "\n",
       "               username  \\\n",
       "794999  quiz_master       \n",
       "795000  djcampos          \n",
       "795001  RKF               \n",
       "795002  AnaHertz          \n",
       "795003  yenafer           \n",
       "...         ...           \n",
       "804994  danalynbyers      \n",
       "804995  joscelinyeo       \n",
       "804996  serengetisunset   \n",
       "804997  broombeck         \n",
       "804998  aptronym          \n",
       "\n",
       "                                                                                                                                              Text  \\\n",
       "794999  Was having dinner with parents downstairs in Dining Room, they started watching 'Baba Ramdev Yoga' thingy, so I'm back to my room  .         \n",
       "795000  Blah 5am still up  daang I got deep problems                                                                                                 \n",
       "795001  @jenspeedy I would suggest avoiding 360 Living.  Not goodness    Try contacting Scott at MKCC mkccrenovations@rogers.com 905-303-9009        \n",
       "795002  @alexbroun I didn't convince myself I was fat and ugly someone else did a pretty good job of that. Its a long story sorry                    \n",
       "795003  @spotzle @jstarrh check on sunscreen, snacks, towels, suits, kids  drinks, bags and tape for boys cast, camera.  Need toys, chairs ugh :-/   \n",
       "...                                                                                                                                            ...   \n",
       "804994  @lbran, thanks for sending us the package - got it this morning!                                                                             \n",
       "804995  @ickleoriental hahahha.. U obviously don't hv one!! But maybe u can give me advice?  Fdw.. Foreign domestic worker                           \n",
       "804996  @juliekoh It's an internet term, but it's spilled over into common use, in real life                                                         \n",
       "804997  new day.... NEW TRACK!!!!                                                                                                                    \n",
       "804998  @foodieguide Okay we need to have a competition to see whether you or I have the dodgiest Cantonese. I bet I win                             \n",
       "\n",
       "                                                                                                                                        text_clean  \\\n",
       "794999  Was having dinner with parents downstairs in Dining Room, they started watching 'Baba Ramdev Yoga' thingy, so I'm back to my room  .         \n",
       "795000  Blah 5am still up  daang I got deep problems                                                                                                 \n",
       "795001  @jenspeedy I would suggest avoiding 360 Living.  Not goodness    Try contacting Scott at MKCC mkccrenovations@rogers.com 905-303-9009        \n",
       "795002  @alexbroun I didn't convince myself I was fat and ugly someone else did a pretty good job of that. Its a long story sorry                    \n",
       "795003  @spotzle @jstarrh check on sunscreen, snacks, towels, suits, kids  drinks, bags and tape for boys cast, camera.  Need toys, chairs ugh :-/   \n",
       "...                                                                                                                                            ...   \n",
       "804994  @lbran, thanks for sending us the package - got it this morning!                                                                             \n",
       "804995  @ickleoriental hahahha.. U obviously don't hv one!! But maybe u can give me advice?  Fdw.. Foreign domestic worker                           \n",
       "804996  @juliekoh It's an internet term, but it's spilled over into common use, in real life                                                         \n",
       "804997  new day.... NEW TRACK!!!!                                                                                                                    \n",
       "804998  @foodieguide Okay we need to have a competition to see whether you or I have the dodgiest Cantonese. I bet I win                             \n",
       "\n",
       "                                                                                                                            text_normalized  \n",
       "794999  dinner parents downstairs dining room started watching baba ramdev yoga ' thingy , ' back room .                                     \n",
       "795000  blah 5am still daang got deep problems                                                                                               \n",
       "795001  jenspeedy would suggest avoiding 360 living goodness try contacting scott mkcc mkccrenovations @ rogers . com 905 - 303 - 9009       \n",
       "795002  alexbroun didnt convince fat ugly someone else pretty good job . long story sorry                                                    \n",
       "795003  spotzle jstarrh check sunscreen , snacks , towels , suits , kids drinks , bags tape boys cast , camera . need toys , chairs ugh :-/  \n",
       "...                                                                                                                                     ...  \n",
       "804994  lbran thanks sending us package - got morning !                                                                                      \n",
       "804995  ickleoriental hahahha . u obviously ' hv one !! maybe u give advice ? fdw .. foreign domestic worker                                 \n",
       "804996  juliekoh internet term , ' spilled common use , real life                                                                            \n",
       "804997  new day .. new track !!!!                                                                                                            \n",
       "804998  foodieguide okay need competition see whether dodgiest cantonese bet win                                                             \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sherilynmoon unfortunate signal tweet photos phone whilst wales',\n",
       "       'dropped car get brakes looked hope doesnt cost insane amounts money',\n",
       "       'colin kelly clyde1 get one republic neyo im school unwell', ...,\n",
       "       'zenaweist could also tweet beccaroberts',\n",
       "       \"good lord still 125 work emails catch actually read ' teach go vacation .\",\n",
       "       'gig northampton racehorse tmw night'], dtype='<U191')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = np.array(df['selftext_clean'])\n",
    "norm_texts = texts[40000:60000]\n",
    "normalized_texts = normalize_corpus(norm_texts)\n",
    "normalized_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Afinn\n",
    "\n",
    "As a quick and dirty sanity check, I've set up Afinn in the early stages of data cleaning, and intend to keep a little record of Afinn's performance, as I increase the rigour of the data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.array(df['selftext_clean'])\n",
    "sentiments = np.array(df['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "train_texts = texts[:10000]\n",
    "train_sentiments = sentiments[:10000]\n",
    "\n",
    "test_texts = texts[40000:60000]\n",
    "test_sentiments = sentiments[40000:60000]\n",
    "sample_review_ids = [5626, 3533, 6010, 123, 2654, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: hot\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n",
      "TEXT: 5dollardinners georgegmithjr pics must change - everytime refresh new pic ! ( always shoes )\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n",
      "TEXT: want twitter fone\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n",
      "TEXT: watching tennis sam stosur playing well today\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n",
      "TEXT: spilled cup coffee embarassing\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n",
      "TEXT: work studying / want enjoy radiant sun\n",
      "Actual Sentiment: 0\n",
      "Predicted Sentiment polarity: 3.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for selftext_clean, sentiment in zip(normalized_texts[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('TEXT:', selftext_clean)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', afn.score(Text))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment with Afinn\n",
    "\n",
    "sentiment_polarity = [afn.score(Text) for Text in normalized_texts]\n",
    "#predicted_sentiments = ['positive' if score >= 1.0 else 'negative' for score in sentiment_polarity]\n",
    "predicted_sentiments = [4 if score >= 1.0 else 0 for score in sentiment_polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.632\n",
      "Precision: 0.635\n",
      "Recall: 0.632\n",
      "F1 Score: 0.6299\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.66      0.56      0.60     10001\n",
      "           0       0.61      0.71      0.66      9999\n",
      "\n",
      "    accuracy                           0.63     20000\n",
      "   macro avg       0.64      0.63      0.63     20000\n",
      "weighted avg       0.64      0.63      0.63     20000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:      \n",
      "                   4     0\n",
      "Actual: 4  5568       4433\n",
      "        0  2928       7071\n"
     ]
    }
   ],
   "source": [
    "#meu.display_model_performance_metrics(true_labels=test_texts, predicted_labels=predicted_sentiments, \n",
    "#                                  classes=['positive', 'negative'])\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=[4, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking cleaning with Afinn\n",
    "\n",
    "I'm curious about how deeper cleaning affects predicitive models. So I set up Afinn after the very first round of data cleaning, and am going to track results here in the markdown. For simplicity, I will monitor the effects of different levels of cleaning on \"weighted avg f1-score\"\n",
    "\n",
    "Round 1, most basic cleaning, 20000 rows:  0.63\n",
    "\n",
    "Round 2, include normalization, 20000 rows: 0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-0fa292de2481>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcv_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcv_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcv_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(test_texts)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000r0cx</th>\n",
       "      <th>0007</th>\n",
       "      <th>000th</th>\n",
       "      <th>00am</th>\n",
       "      <th>01</th>\n",
       "      <th>01000101</th>\n",
       "      <th>01614948343</th>\n",
       "      <th>018</th>\n",
       "      <th>01yt</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>...</th>\n",
       "      <th>½o</th>\n",
       "      <th>½quiï</th>\n",
       "      <th>½r</th>\n",
       "      <th>½re</th>\n",
       "      <th>½rmï</th>\n",
       "      <th>½s</th>\n",
       "      <th>½se</th>\n",
       "      <th>½stand</th>\n",
       "      <th>½t</th>\n",
       "      <th>½tï</th>\n",
       "      <th>½ve</th>\n",
       "      <th>½vel</th>\n",
       "      <th>½y</th>\n",
       "      <th>½ï</th>\n",
       "      <th>ã¼ã</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 29867 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  0000r0cx  0007  000th  00am  01  01000101  01614948343  018  \\\n",
       "0      0   0    0         0     0      0     0   0         0            0     \n",
       "1      0   0    0         0     0      0     0   0         0            0     \n",
       "2      0   0    0         0     0      0     0   0         0            0     \n",
       "3      0   0    0         0     0      0     0   0         0            0     \n",
       "4      0   0    0         0     0      0     0   0         0            0     \n",
       "...   ..  ..   ..        ..    ..     ..    ..  ..        ..           ..     \n",
       "19995  0   0    0         0     0      0     0   0         0            0     \n",
       "19996  0   0    0         0     0      0     0   0         0            0     \n",
       "19997  0   0    0         0     0      0     0   0         0            0     \n",
       "19998  0   0    0         0     0      0     0   0         0            0     \n",
       "19999  0   0    0         0     0      0     0   0         0            0     \n",
       "\n",
       "       01yt  02  03  04  05  ...  ½o  ½quiï  ½r  ½re  ½rmï  ½s  ½se  ½stand  \\\n",
       "0      0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "1      0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "2      0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "3      0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "4      0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "...   ..    ..  ..  ..  ..   ... ..  ..     ..  ..   ..    ..  ..   ..        \n",
       "19995  0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "19996  0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "19997  0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "19998  0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "19999  0     0   0   0   0   ...  0   0      0   0    0     0   0    0        \n",
       "\n",
       "       ½t  ½tï  ½ve  ½vel  ½y  ½ï  ã¼ã  \n",
       "0      0   0    0    0     0   0   0    \n",
       "1      0   0    0    0     0   0   0    \n",
       "2      0   0    0    0     0   0   0    \n",
       "3      0   0    0    0     0   0   0    \n",
       "4      0   0    0    0     0   0   0    \n",
       "...   ..  ..   ..   ..    ..  ..  ..    \n",
       "19995  0   0    0    0     0   0   0    \n",
       "19996  0   0    0    0     0   0   0    \n",
       "19997  0   0    0    0     0   0   0    \n",
       "19998  0   0    0    0     0   0   0    \n",
       "19999  0   0    0    0     0   0   0    \n",
       "\n",
       "[20000 rows x 29867 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all unique words in the corpus\n",
    "vocab = cv.get_feature_names()\n",
    "# show document feature vectors\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L75iqChDnSCe"
   },
   "source": [
    "#### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynhqSwDunSCe"
   },
   "source": [
    "## Load spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMNfxFA1nSCe"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1l-paPPnSCe"
   },
   "source": [
    "## Iterate over all rows and perform NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "7IGiQ5hfnSCe",
    "outputId": "5170760f-a771-4c10-8a66-51fc279ba9d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750000\n",
      "751000\n",
      "752000\n",
      "753000\n",
      "754000\n",
      "755000\n",
      "756000\n",
      "757000\n",
      "758000\n",
      "759000\n",
      "760000\n",
      "761000\n",
      "762000\n",
      "763000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1c734ccf6eaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"selftext_clean\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"selftext_clean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"selftext_clean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0madjectives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mnouns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mXhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36m_get_moments\u001b[1;34m(ops, X)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_get_moments_reproduce_bug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-08\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"f\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if(row[\"selftext_clean\"] and len(str(row[\"selftext_clean\"])) < 1000000):\n",
    "        doc = nlp(str(row[\"selftext_clean\"]))\n",
    "        adjectives = []\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        lemmas = []\n",
    "\n",
    "        for token in doc:\n",
    "            lemmas.append(token.lemma_)\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "                nouns.append(token.lemma_)\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_)\n",
    "                \n",
    "        df.at[i, \"selftext_lemma\"] = \" \".join(lemmas)                \n",
    "        df.at[i, \"selftext_nouns\"] = \" \".join(nouns)\n",
    "        df.at[i, \"selftext_adjectives\"] = \" \".join(adjectives)\n",
    "        df.at[i, \"selftext_verbs\"] = \" \".join(verbs)\n",
    "        df.at[i, \"selftext_nav\"] = \" \".join(nouns+adjectives+verbs)\n",
    "        df.at[i, \"no_tokens\"] = len(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQdL3PkynSCo"
   },
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "I9_JnCudnSCo",
    "outputId": "e6b101dc-9e29-49c5-e721-777e4557609a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Time</th>\n",
       "      <th>none</th>\n",
       "      <th>username</th>\n",
       "      <th>Text</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>selftext_lemma</th>\n",
       "      <th>selftext_nouns</th>\n",
       "      <th>selftext_adjectives</th>\n",
       "      <th>selftext_verbs</th>\n",
       "      <th>selftext_nav</th>\n",
       "      <th>no_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>750000</td>\n",
       "      <td>0</td>\n",
       "      <td>2285370823</td>\n",
       "      <td>Mon Jun 22 15:02:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xbeautifulmessx</td>\n",
       "      <td>@Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.</td>\n",
       "      <td>@Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.</td>\n",
       "      <td>@Idristwilight -PRON- can post HAN when -PRON- want . -PRON- be great ! -PRON- be still work on TLD though . -PRON- get a little distracted so sorry .</td>\n",
       "      <td>@Idristwilight HAN TLD</td>\n",
       "      <td>great distracted sorry</td>\n",
       "      <td>can post want work get</td>\n",
       "      <td>@Idristwilight HAN TLD great distracted sorry can post want work get</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750001</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371185</td>\n",
       "      <td>Mon Jun 22 15:02:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thefirstsight</td>\n",
       "      <td>@rose_7 Ohh poor jan  please tell her that if she cans, send us an email!!</td>\n",
       "      <td>@rose 7 Ohh poor jan  please tell her that if she cans, send us an email!!</td>\n",
       "      <td>@rose 7 Ohh poor jan   please tell -PRON- that if -PRON- can , send -PRON- an email ! !</td>\n",
       "      <td>Ohh jan email</td>\n",
       "      <td>poor</td>\n",
       "      <td>tell can send</td>\n",
       "      <td>Ohh jan email poor tell can send</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750002</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371495</td>\n",
       "      <td>Mon Jun 22 15:02:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Sarah2713</td>\n",
       "      <td>Finally home from work...It was a looong day!! And it's only Monday</td>\n",
       "      <td>Finally home from work...It was a looong day!! And it's only Monday</td>\n",
       "      <td>finally home from work ... -PRON- be a looong day ! ! and -PRON- be only Monday</td>\n",
       "      <td>work day Monday</td>\n",
       "      <td>looong</td>\n",
       "      <td></td>\n",
       "      <td>work day Monday looong</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750003</td>\n",
       "      <td>0</td>\n",
       "      <td>2285371762</td>\n",
       "      <td>Mon Jun 22 15:02:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dierockerfrau</td>\n",
       "      <td>im very sad 4 chantelle and tom</td>\n",
       "      <td>im very sad 4 chantelle and tom</td>\n",
       "      <td>-PRON- be very sad 4 chantelle and tom</td>\n",
       "      <td>chantelle tom</td>\n",
       "      <td>sad</td>\n",
       "      <td>be</td>\n",
       "      <td>chantelle tom sad be</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750004</td>\n",
       "      <td>0</td>\n",
       "      <td>2285372377</td>\n",
       "      <td>Mon Jun 22 15:02:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>alexbates</td>\n",
       "      <td>I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste</td>\n",
       "      <td>I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste</td>\n",
       "      <td>-PRON- chat with someone on the online Apple store and -PRON- say -PRON- would be well to buy a new one . -PRON- do not have $ 200 to waste</td>\n",
       "      <td>Apple store</td>\n",
       "      <td>online well new</td>\n",
       "      <td>chat say would buy waste</td>\n",
       "      <td>Apple store online well new chat say would buy waste</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          ID                          Time      none  \\\n",
       "750000  0          2285370823  Mon Jun 22 15:02:49 PDT 2009  NO_QUERY   \n",
       "750001  0          2285371185  Mon Jun 22 15:02:51 PDT 2009  NO_QUERY   \n",
       "750002  0          2285371495  Mon Jun 22 15:02:52 PDT 2009  NO_QUERY   \n",
       "750003  0          2285371762  Mon Jun 22 15:02:54 PDT 2009  NO_QUERY   \n",
       "750004  0          2285372377  Mon Jun 22 15:02:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "               username  \\\n",
       "750000  xbeautifulmessx   \n",
       "750001  thefirstsight     \n",
       "750002  Sarah2713         \n",
       "750003  dierockerfrau     \n",
       "750004  alexbates         \n",
       "\n",
       "                                                                                                                                     Text  \\\n",
       "750000  @Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.    \n",
       "750001  @rose_7 Ohh poor jan  please tell her that if she cans, send us an email!!                                                          \n",
       "750002  Finally home from work...It was a looong day!! And it's only Monday                                                                 \n",
       "750003  im very sad 4 chantelle and tom                                                                                                     \n",
       "750004  I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste      \n",
       "\n",
       "                                                                                                                           selftext_clean  \\\n",
       "750000  @Idristwilight You can post HAN when you want. It's great! I am still working on TLD though. I got a little distracted so sorry.    \n",
       "750001  @rose 7 Ohh poor jan  please tell her that if she cans, send us an email!!                                                          \n",
       "750002  Finally home from work...It was a looong day!! And it's only Monday                                                                 \n",
       "750003  im very sad 4 chantelle and tom                                                                                                     \n",
       "750004  I chatted with someone on the online Apple store and they said it would be better to buy a new one. I don't have $200 to waste      \n",
       "\n",
       "                                                                                                                                                selftext_lemma  \\\n",
       "750000  @Idristwilight -PRON- can post HAN when -PRON- want . -PRON- be great ! -PRON- be still work on TLD though . -PRON- get a little distracted so sorry .   \n",
       "750001  @rose 7 Ohh poor jan   please tell -PRON- that if -PRON- can , send -PRON- an email ! !                                                                  \n",
       "750002  finally home from work ... -PRON- be a looong day ! ! and -PRON- be only Monday                                                                          \n",
       "750003  -PRON- be very sad 4 chantelle and tom                                                                                                                   \n",
       "750004  -PRON- chat with someone on the online Apple store and -PRON- say -PRON- would be well to buy a new one . -PRON- do not have $ 200 to waste              \n",
       "\n",
       "                selftext_nouns     selftext_adjectives  \\\n",
       "750000  @Idristwilight HAN TLD  great distracted sorry   \n",
       "750001  Ohh jan email           poor                     \n",
       "750002  work day Monday         looong                   \n",
       "750003  chantelle tom           sad                      \n",
       "750004  Apple store             online well new          \n",
       "\n",
       "                  selftext_verbs  \\\n",
       "750000  can post want work get     \n",
       "750001  tell can send              \n",
       "750002                             \n",
       "750003  be                         \n",
       "750004  chat say would buy waste   \n",
       "\n",
       "                                                                selftext_nav  \\\n",
       "750000  @Idristwilight HAN TLD great distracted sorry can post want work get   \n",
       "750001  Ohh jan email poor tell can send                                       \n",
       "750002  work day Monday looong                                                 \n",
       "750003  chantelle tom sad be                                                   \n",
       "750004  Apple store online well new chat say would buy waste                   \n",
       "\n",
       "        no_tokens  \n",
       "750000 29.00       \n",
       "750001 20.00       \n",
       "750002 17.00       \n",
       "750003 8.00        \n",
       "750004 31.00       "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aa6qxMhqnSCo"
   },
   "source": [
    "## Save to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFdiqd0WnSCy"
   },
   "outputs": [],
   "source": [
    "df.to_sql('posts_nlp', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWaPk57LqDRV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tImPFlPIqG5f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "csml1010-pete-gray.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
